{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cathedral_rl import cathedral_v0  \n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device : {device}')\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "rewards : {'player_0': np.int64(0), 'player_1': 0}\n",
      "cumulative reward : {'player_0': np.int64(0), 'player_1': 0}\n",
      "=============================================\n",
      "rewards : {'player_0': np.int64(0), 'player_1': np.int64(0)}\n",
      "cumulative reward : {'player_0': np.int64(0), 'player_1': np.int64(0)}\n",
      "=============================================\n",
      "rewards : {'player_0': np.int64(-2), 'player_1': np.int64(0)}\n",
      "cumulative reward : {'player_0': np.int64(-2), 'player_1': np.int64(0)}\n",
      "=============================================\n",
      "rewards : {'player_0': np.int64(-2), 'player_1': np.int64(-4)}\n",
      "cumulative reward : {'player_0': np.int64(-4), 'player_1': np.int64(-4)}\n",
      "=============================================\n",
      "rewards : {'player_0': np.int64(-2), 'player_1': np.int64(-4)}\n",
      "cumulative reward : {'player_0': np.int64(-6), 'player_1': np.int64(-8)}\n",
      "=============================================\n",
      "rewards : {'player_0': np.int64(-2), 'player_1': np.int64(-2)}\n",
      "cumulative reward : {'player_0': np.int64(-8), 'player_1': np.int64(-10)}\n",
      "=============================================\n",
      "rewards : {'player_0': np.int64(-4), 'player_1': np.int64(-2)}\n",
      "cumulative reward : {'player_0': np.int64(-12), 'player_1': np.int64(-12)}\n",
      "=============================================\n",
      "rewards : {'player_0': np.int64(-4), 'player_1': np.int64(-1)}\n",
      "cumulative reward : {'player_0': np.int64(-16), 'player_1': np.int64(-13)}\n",
      "=============================================\n",
      "rewards : {'player_0': np.int64(-4), 'player_1': np.int64(-1)}\n",
      "cumulative reward : {'player_0': np.int64(-20), 'player_1': np.int64(-14)}\n",
      "=============================================\n",
      "rewards : {'player_0': np.int64(-4), 'player_1': np.int64(-3)}\n",
      "cumulative reward : {'player_0': np.int64(-24), 'player_1': np.int64(-17)}\n",
      "=============================================\n",
      "rewards : {'player_0': np.int64(-3), 'player_1': np.int64(-3)}\n",
      "cumulative reward : {'player_0': np.int64(-27), 'player_1': np.int64(-20)}\n",
      "=============================================\n",
      "rewards : {'player_0': np.int64(-3), 'player_1': np.int64(-4)}\n",
      "cumulative reward : {'player_0': np.int64(-30), 'player_1': np.int64(-24)}\n",
      "=============================================\n",
      "rewards : {'player_0': np.int64(-2), 'player_1': np.int64(-4)}\n",
      "cumulative reward : {'player_0': np.int64(-32), 'player_1': np.int64(-28)}\n",
      "=============================================\n",
      "rewards : {'player_0': np.int64(-2), 'player_1': np.int64(-1)}\n",
      "cumulative reward : {'player_0': np.int64(-34), 'player_1': np.int64(-29)}\n",
      "=============================================\n",
      "rewards : {'player_0': -2, 'player_1': np.int64(5)}\n",
      "cumulative reward : {'player_0': np.int64(-36), 'player_1': np.int64(-24)}\n",
      "=============================================\n",
      "La partie est terminée.\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "env = cathedral_v0.env(board_size=7, render_mode=\"text\", per_move_rewards=True, final_reward_score_difference=True)\n",
    "\n",
    "env.reset()\n",
    "count = 0\n",
    "print('=============================================')\n",
    "while env.agents:\n",
    "    agent = env.agent_selection\n",
    "    \n",
    "    observation = env.observe(agent)\n",
    "    \n",
    "    # obs = observation[\"observation\"]\n",
    "    # for i in range(obs.shape[2]):\n",
    "    #     print(f\"Canal {i+1} :\")\n",
    "    #     print(obs[:, :, i])\n",
    "    #     print(\"\\n\")\n",
    "    \n",
    "\n",
    "    legal_moves = [i for i, valid in enumerate(observation[\"action_mask\"]) if valid]\n",
    "    # print(f'legale move {len(legal_moves)}')\n",
    "    # print(f\"{agent}. Actions légales : {legal_moves}\")\n",
    "    \n",
    "    action = random.choice(legal_moves)\n",
    "    # print(f\"{agent} joue l'action {action}.\")\n",
    "    \n",
    "    env.step(action)\n",
    "    \n",
    "    # print(\"==RENDER==\")\n",
    "    # env.render()\n",
    "    \n",
    "    print(f'rewards : {env.rewards}')\n",
    "    print(f'cumulative reward : {env._cumulative_rewards}')\n",
    "    print('=============================================')\n",
    "    count += 1\n",
    "\n",
    "print(\"La partie est terminée.\")\n",
    "print(count)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done, action_mask, next_action_mask):\n",
    "        self.buffer.append((state, action, reward, next_state, done, action_mask, next_action_mask))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done, action_mask, next_action_mask = map(np.array, zip(*batch))\n",
    "        return state, action, reward, next_state, done, action_mask, next_action_mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # observations : (10, 10, 5)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(obs_shape[2], 32, kernel_size=3, stride=1, padding=1),  # output: 32 x 10 x 10\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),              # output: 64 x 10 x 10\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        dummy = torch.zeros(1, obs_shape[2], obs_shape[0], obs_shape[1])\n",
    "        conv_out_size = self.conv(dummy).shape[1]\n",
    "        print(f'conv_out_size : {conv_out_size}')\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x(batch, hauteur, largeur, channels)\n",
    "        x = x.permute(0, 3, 1, 2)  \n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_obs(obs):\n",
    "    return torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "def select_action_dqn(model, obs, action_mask, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        obs_tensor = preprocess_obs(obs).unsqueeze(0).to(device)  # (1, 10, 10, 5)\n",
    "        q_values = model(obs_tensor).squeeze(0)  # (n_actions,)\n",
    "        mask = torch.tensor(action_mask, dtype=torch.bool, device=device)\n",
    "        # Masquer les actions illégales en assignant -inf à leurs Q-valeurs\n",
    "        q_values[~mask] = -1e8\n",
    "        action = torch.argmax(q_values).item()\n",
    "    model.train()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state_for_controlled(env, controlled_agent, current_state, current_mask):\n",
    "    \"\"\"\n",
    "    Simule les actions des autres agents jusqu'à ce que controlled_agent ait de nouveau la main.\n",
    "    Retourne : next_state, next_action_mask et done_flag.\n",
    "    \"\"\"\n",
    "    # On avance tant que controlled_agent n'est pas actif et que la partie n'est pas terminée.\n",
    "    while env.agent_selection != controlled_agent and env.agents:\n",
    "        current_agent = env.agent_selection\n",
    "        obs = env.observe(current_agent)\n",
    "        legal_moves = [i for i, valid in enumerate(obs[\"action_mask\"]) if valid]\n",
    "        if not legal_moves:\n",
    "            env.step(0)\n",
    "        else:\n",
    "            env.step(random.choice(legal_moves))\n",
    "            \n",
    "    if controlled_agent in env.agents:\n",
    "        next_obs = env.observe(controlled_agent)\n",
    "        next_state = next_obs[\"observation\"]\n",
    "        next_action_mask = next_obs[\"action_mask\"]\n",
    "        done_flag = 0\n",
    "    else:\n",
    "        next_state = np.zeros_like(current_state)\n",
    "        next_action_mask = np.zeros_like(current_mask)\n",
    "        done_flag = 1\n",
    "    return next_state, next_action_mask, done_flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Hyperparamètres ---\n",
    "num_episodes = 100\n",
    "buffer_capacity = 1000\n",
    "batch_size = 64\n",
    "gamma = 0.999\n",
    "learning_rate = 1e-3\n",
    "board_size = 8\n",
    "\n",
    "updates = 10 # On effectue plusieurs mises à jour par épisode\n",
    "target_update_freq = 10    # fréquence (en épisodes) de mise à jour du réseau cible\n",
    "\n",
    "epsilon_start = 0.2\n",
    "epsilon_final = 0.05\n",
    "epsilon_decay = 100      \n",
    "\n",
    "controlled_agent = \"player_0\" \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device : {device}')\n",
    "\n",
    "\n",
    "def epsilon_by_episode(episode):\n",
    "    return epsilon_final + (epsilon_start - epsilon_final) * np.exp(-episode / epsilon_decay)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(name):\n",
    "    env = cathedral_v0.env(board_size=board_size, render_mode=\"text\", per_move_rewards=True, final_reward_score_difference=False)\n",
    "    env.reset()\n",
    "    enter_train = False\n",
    "    n_actions = env.action_space(controlled_agent).n\n",
    "    print(f'n_actions : {n_actions}')\n",
    "    obs_shape = env.observe(controlled_agent)[\"observation\"].shape  # (10, 10, 5)\n",
    "\n",
    "    policy_net = DQN(obs_shape, n_actions).to(device)\n",
    "    target_net = DQN(obs_shape, n_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "    replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "    list_reward = []\n",
    "    list_epsilon = []\n",
    "    win_count = 0\n",
    "    for episode in range(num_episodes):\n",
    "        env.reset()\n",
    "        total_reward = 0\n",
    "        losses = []\n",
    "        while env.agents:\n",
    "            current_agent = env.agent_selection\n",
    "            observation = env.observe(current_agent)\n",
    "            legal_moves = [i for i, valid in enumerate(observation[\"action_mask\"]) if valid]\n",
    "            \n",
    "            if current_agent == controlled_agent:\n",
    "                state = observation[\"observation\"]\n",
    "                action_mask = observation[\"action_mask\"]\n",
    "                # print(f'ACTION MASK : {np.any(action_mask==1)}\\n{action_mask.shape}\\n{action_mask}')\n",
    "                    \n",
    "                epsilon = epsilon_by_episode(episode) # epsilon-greedy\n",
    "                list_epsilon.append(epsilon)\n",
    "                if random.random() < epsilon:\n",
    "                    action = random.choice(legal_moves)\n",
    "                else:\n",
    "                    action = select_action_dqn(policy_net, state, action_mask, device)\n",
    "                \n",
    "                # print(f'action : {action}')\n",
    "                \n",
    "                env.step(action)\n",
    "                reward = env.rewards[current_agent]\n",
    "                total_reward += reward\n",
    "\n",
    "                next_state, next_action_mask, done_flag = get_next_state_for_controlled(env, controlled_agent, state, action_mask)\n",
    "                \n",
    "                \n",
    "                replay_buffer.push(state, action, reward, next_state, done_flag, action_mask, next_action_mask)\n",
    "                \n",
    "            else:\n",
    "                action = random.choice(legal_moves)\n",
    "                env.step(action)\n",
    "        \n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                enter_train = True\n",
    "                \n",
    "                for _ in range(updates):  \n",
    "                    states, actions, rewards, next_states, dones, action_masks, next_action_masks = replay_buffer.sample(batch_size)\n",
    "                    \n",
    "                    states_tensor = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "                    actions_tensor = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "                    rewards_tensor = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "                    next_states_tensor = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "                    dones_tensor = torch.tensor(dones, dtype=torch.float32).to(device)\n",
    "                    next_action_masks_tensor = torch.tensor(next_action_masks, dtype=torch.bool).to(device)\n",
    "                    \n",
    "                    # Q-valeurs actuelles pour les actions sélectionnées\n",
    "                    q_values = policy_net(states_tensor)\n",
    "                    # print(f'q_values : {q_values.shape}\\n{q_values}')\n",
    "                    q_values = q_values.gather(1, actions_tensor.unsqueeze(1)).squeeze(1)\n",
    "                    # print(f'q_values gather: {q_values.shape}\\n{q_values}')\n",
    "                    \n",
    "                    # Calcul des Q-valeurs cibles via le réseau cible\n",
    "                    with torch.no_grad():\n",
    "                        next_q_values = target_net(next_states_tensor)  # shape: [batch_size, n_actions]\n",
    "        \n",
    "                        # On remplace les actions illégales par une grande valeur négative, mais finie.\n",
    "                        next_q_values_masked = next_q_values.clone()\n",
    "                        next_q_values_masked[~next_action_masks_tensor] = -1e8\n",
    "                        \n",
    "                        # Pour chaque échantillon, si aucune action n'est légale, on fixe le max à 0.\n",
    "                        mask_sum = next_action_masks_tensor.sum(dim=1)\n",
    "                        max_next_q_values = next_q_values_masked.max(1)[0]\n",
    "                        max_next_q_values[mask_sum == 0] = 0.0\n",
    "\n",
    "                        target_q_values = rewards_tensor + gamma * max_next_q_values * (1 - dones_tensor)\n",
    "                        \n",
    "                    loss = nn.MSELoss()(q_values, target_q_values)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    losses.append(loss.item())\n",
    "                    \n",
    "        list_reward.append(total_reward)\n",
    "        if env.winner == 0:  # controlled_agent (\"player_0\") gagne\n",
    "            win_count += 1\n",
    "        elif env.winner == -1:  # match nul : on peut compter 0.5 victoire\n",
    "            win_count += 0.5\n",
    "        \n",
    "        \n",
    "        if enter_train:\n",
    "            winner = env.winner\n",
    "            print(f\"Episode {episode+1}/{num_episodes} - Reward total: {total_reward:.2f} - Loss: {sum(losses)/len(losses):.4f} - Winner: {winner} - Epsilon: {epsilon_by_episode(episode):.2f}\")\n",
    "        if (episode+1) % target_update_freq == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            print(\"Update target_net\")\n",
    "    env.close()\n",
    "\n",
    "    # torch.save(policy_net.state_dict(), f\"model_weights_DQN/{name}.pth\")\n",
    "    torch.save({\n",
    "        'model_state_dict': policy_net.state_dict(),\n",
    "        'list_reward': list_reward,\n",
    "        'list_epsilon': list_epsilon\n",
    "    }, f\"model_weights_DQN/{name}.pth\")\n",
    "    \n",
    "    print(f'Winrate : {win_count/num_episodes}')\n",
    "    \n",
    "    return list_reward, list_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_actions : 1753\n",
      "conv_out_size : 4096\n",
      "conv_out_size : 4096\n",
      "Episode 7/100 - Reward total: -7.00 - Loss: 0.8255 - Winner: 0 - Epsilon: 0.19\n",
      "Episode 8/100 - Reward total: -14.00 - Loss: 0.0678 - Winner: 1 - Epsilon: 0.19\n",
      "Episode 9/100 - Reward total: -19.00 - Loss: 0.0593 - Winner: 1 - Epsilon: 0.19\n",
      "Episode 10/100 - Reward total: -16.00 - Loss: 0.0309 - Winner: 1 - Epsilon: 0.19\n",
      "Update target_net\n",
      "Episode 11/100 - Reward total: -13.00 - Loss: 0.2623 - Winner: 1 - Epsilon: 0.19\n",
      "Episode 12/100 - Reward total: -9.00 - Loss: 0.0299 - Winner: 0 - Epsilon: 0.18\n",
      "Episode 13/100 - Reward total: -17.00 - Loss: 0.0248 - Winner: 1 - Epsilon: 0.18\n",
      "Episode 14/100 - Reward total: -15.00 - Loss: 0.0276 - Winner: 0 - Epsilon: 0.18\n",
      "Episode 15/100 - Reward total: -15.00 - Loss: 0.0329 - Winner: 1 - Epsilon: 0.18\n",
      "Episode 16/100 - Reward total: -9.00 - Loss: 0.0152 - Winner: 0 - Epsilon: 0.18\n",
      "Episode 17/100 - Reward total: -12.00 - Loss: 0.0265 - Winner: 1 - Epsilon: 0.18\n",
      "Episode 18/100 - Reward total: -11.00 - Loss: 0.0330 - Winner: 0 - Epsilon: 0.18\n",
      "Episode 19/100 - Reward total: -9.00 - Loss: 0.0473 - Winner: 1 - Epsilon: 0.18\n",
      "Episode 20/100 - Reward total: -12.00 - Loss: 0.0195 - Winner: 1 - Epsilon: 0.17\n",
      "Update target_net\n",
      "Episode 21/100 - Reward total: -6.00 - Loss: 0.1320 - Winner: 0 - Epsilon: 0.17\n",
      "Episode 22/100 - Reward total: 4.00 - Loss: 0.0950 - Winner: 0 - Epsilon: 0.17\n",
      "Episode 23/100 - Reward total: -8.00 - Loss: 0.0620 - Winner: 0 - Epsilon: 0.17\n",
      "Episode 24/100 - Reward total: -4.00 - Loss: 0.1050 - Winner: 0 - Epsilon: 0.17\n",
      "Episode 25/100 - Reward total: -15.00 - Loss: 0.0688 - Winner: 1 - Epsilon: 0.17\n",
      "Episode 26/100 - Reward total: -9.00 - Loss: 0.0637 - Winner: 0 - Epsilon: 0.17\n",
      "Episode 27/100 - Reward total: -12.00 - Loss: 0.0614 - Winner: 1 - Epsilon: 0.17\n",
      "Episode 28/100 - Reward total: -8.00 - Loss: 0.0681 - Winner: 0 - Epsilon: 0.16\n",
      "Episode 29/100 - Reward total: -10.00 - Loss: 0.0671 - Winner: 0 - Epsilon: 0.16\n",
      "Episode 30/100 - Reward total: -2.00 - Loss: 0.0929 - Winner: 0 - Epsilon: 0.16\n",
      "Update target_net\n",
      "Episode 31/100 - Reward total: -12.00 - Loss: 0.2149 - Winner: 1 - Epsilon: 0.16\n",
      "Episode 32/100 - Reward total: -11.00 - Loss: 0.0579 - Winner: 0 - Epsilon: 0.16\n",
      "Episode 33/100 - Reward total: -7.00 - Loss: 0.0736 - Winner: 1 - Epsilon: 0.16\n",
      "Episode 34/100 - Reward total: -8.00 - Loss: 0.0565 - Winner: 0 - Epsilon: 0.16\n",
      "Episode 35/100 - Reward total: -16.00 - Loss: 0.0741 - Winner: 1 - Epsilon: 0.16\n",
      "Episode 36/100 - Reward total: -6.00 - Loss: 0.0503 - Winner: 0 - Epsilon: 0.16\n",
      "Episode 37/100 - Reward total: -19.00 - Loss: 0.0728 - Winner: 1 - Epsilon: 0.15\n",
      "Episode 38/100 - Reward total: -7.00 - Loss: 0.0786 - Winner: 0 - Epsilon: 0.15\n",
      "Episode 39/100 - Reward total: 1.00 - Loss: 0.0817 - Winner: 0 - Epsilon: 0.15\n",
      "Episode 40/100 - Reward total: -8.00 - Loss: 0.0586 - Winner: 0 - Epsilon: 0.15\n",
      "Update target_net\n",
      "Episode 41/100 - Reward total: -11.00 - Loss: 0.1740 - Winner: 0 - Epsilon: 0.15\n",
      "Episode 42/100 - Reward total: -8.00 - Loss: 0.1021 - Winner: -1 - Epsilon: 0.15\n",
      "Episode 43/100 - Reward total: -5.00 - Loss: 0.1318 - Winner: 1 - Epsilon: 0.15\n",
      "Episode 44/100 - Reward total: -9.00 - Loss: 0.1203 - Winner: 0 - Epsilon: 0.15\n",
      "Episode 45/100 - Reward total: -11.00 - Loss: 0.1066 - Winner: 1 - Epsilon: 0.15\n",
      "Episode 46/100 - Reward total: -9.00 - Loss: 0.1032 - Winner: 0 - Epsilon: 0.15\n",
      "Episode 47/100 - Reward total: -3.00 - Loss: 0.1051 - Winner: 0 - Epsilon: 0.14\n",
      "Episode 48/100 - Reward total: -11.00 - Loss: 0.1006 - Winner: -1 - Epsilon: 0.14\n",
      "Episode 49/100 - Reward total: -8.00 - Loss: 0.1071 - Winner: 0 - Epsilon: 0.14\n",
      "Episode 50/100 - Reward total: 1.00 - Loss: 0.0989 - Winner: 0 - Epsilon: 0.14\n",
      "Update target_net\n",
      "Episode 51/100 - Reward total: -14.00 - Loss: 0.3152 - Winner: 1 - Epsilon: 0.14\n",
      "Episode 52/100 - Reward total: -8.00 - Loss: 0.1731 - Winner: 0 - Epsilon: 0.14\n",
      "Episode 53/100 - Reward total: -7.00 - Loss: 0.1646 - Winner: 1 - Epsilon: 0.14\n",
      "Episode 54/100 - Reward total: -4.00 - Loss: 0.1548 - Winner: 0 - Epsilon: 0.14\n",
      "Episode 55/100 - Reward total: -5.00 - Loss: 0.1635 - Winner: 0 - Epsilon: 0.14\n",
      "Episode 56/100 - Reward total: -12.00 - Loss: 0.1625 - Winner: -1 - Epsilon: 0.14\n",
      "Episode 57/100 - Reward total: 5.00 - Loss: 0.1864 - Winner: 0 - Epsilon: 0.14\n",
      "Episode 58/100 - Reward total: -12.00 - Loss: 0.1777 - Winner: 1 - Epsilon: 0.13\n",
      "Episode 59/100 - Reward total: -1.00 - Loss: 0.1413 - Winner: 0 - Epsilon: 0.13\n",
      "Episode 60/100 - Reward total: -9.00 - Loss: 0.1454 - Winner: 0 - Epsilon: 0.13\n",
      "Update target_net\n",
      "Episode 61/100 - Reward total: -7.00 - Loss: 0.3395 - Winner: 0 - Epsilon: 0.13\n",
      "Episode 62/100 - Reward total: -8.00 - Loss: 0.2783 - Winner: 0 - Epsilon: 0.13\n",
      "Episode 63/100 - Reward total: -18.00 - Loss: 0.2745 - Winner: 1 - Epsilon: 0.13\n",
      "Episode 64/100 - Reward total: -8.00 - Loss: 0.2311 - Winner: 0 - Epsilon: 0.13\n",
      "Episode 65/100 - Reward total: -4.00 - Loss: 0.2366 - Winner: 0 - Epsilon: 0.13\n",
      "Episode 66/100 - Reward total: -14.00 - Loss: 0.3005 - Winner: 1 - Epsilon: 0.13\n",
      "Episode 67/100 - Reward total: -1.00 - Loss: 0.2472 - Winner: 0 - Epsilon: 0.13\n",
      "Episode 68/100 - Reward total: -3.00 - Loss: 0.2538 - Winner: 0 - Epsilon: 0.13\n",
      "Episode 69/100 - Reward total: -5.00 - Loss: 0.2267 - Winner: 0 - Epsilon: 0.13\n",
      "Episode 70/100 - Reward total: 4.00 - Loss: 0.2565 - Winner: 0 - Epsilon: 0.13\n",
      "Update target_net\n",
      "Episode 71/100 - Reward total: 0.00 - Loss: 0.3872 - Winner: 0 - Epsilon: 0.12\n",
      "Episode 72/100 - Reward total: -13.00 - Loss: 0.3071 - Winner: 0 - Epsilon: 0.12\n",
      "Episode 73/100 - Reward total: -11.00 - Loss: 0.3297 - Winner: 0 - Epsilon: 0.12\n",
      "Episode 74/100 - Reward total: -9.00 - Loss: 0.3452 - Winner: 0 - Epsilon: 0.12\n",
      "Episode 75/100 - Reward total: -10.00 - Loss: 0.2929 - Winner: 0 - Epsilon: 0.12\n",
      "Episode 76/100 - Reward total: -15.00 - Loss: 0.3005 - Winner: -1 - Epsilon: 0.12\n",
      "Episode 77/100 - Reward total: -4.00 - Loss: 0.2687 - Winner: 0 - Epsilon: 0.12\n",
      "Episode 78/100 - Reward total: -15.00 - Loss: 0.2977 - Winner: 1 - Epsilon: 0.12\n",
      "Episode 79/100 - Reward total: -5.00 - Loss: 0.3403 - Winner: 0 - Epsilon: 0.12\n",
      "Episode 80/100 - Reward total: -15.00 - Loss: 0.3332 - Winner: 1 - Epsilon: 0.12\n",
      "Update target_net\n",
      "Episode 81/100 - Reward total: 1.00 - Loss: 0.6056 - Winner: 0 - Epsilon: 0.12\n",
      "Episode 82/100 - Reward total: -9.00 - Loss: 0.5088 - Winner: 0 - Epsilon: 0.12\n",
      "Episode 83/100 - Reward total: -2.00 - Loss: 0.4797 - Winner: 0 - Epsilon: 0.12\n",
      "Episode 84/100 - Reward total: -11.00 - Loss: 0.4412 - Winner: 0 - Epsilon: 0.12\n",
      "Episode 85/100 - Reward total: -1.00 - Loss: 0.5002 - Winner: 0 - Epsilon: 0.11\n",
      "Episode 86/100 - Reward total: -4.00 - Loss: 0.4794 - Winner: 0 - Epsilon: 0.11\n",
      "Episode 87/100 - Reward total: 6.00 - Loss: 0.4938 - Winner: 0 - Epsilon: 0.11\n",
      "Episode 88/100 - Reward total: 2.00 - Loss: 0.4373 - Winner: 0 - Epsilon: 0.11\n",
      "Episode 89/100 - Reward total: -14.00 - Loss: 0.4755 - Winner: 1 - Epsilon: 0.11\n",
      "Episode 90/100 - Reward total: 1.00 - Loss: 0.4550 - Winner: 0 - Epsilon: 0.11\n",
      "Update target_net\n",
      "Episode 91/100 - Reward total: 5.00 - Loss: 0.5155 - Winner: 0 - Epsilon: 0.11\n",
      "Episode 92/100 - Reward total: -12.00 - Loss: 0.4879 - Winner: 1 - Epsilon: 0.11\n",
      "Episode 93/100 - Reward total: 0.00 - Loss: 0.4317 - Winner: 0 - Epsilon: 0.11\n",
      "Episode 94/100 - Reward total: 7.00 - Loss: 0.4137 - Winner: 0 - Epsilon: 0.11\n",
      "Episode 95/100 - Reward total: 3.00 - Loss: 0.4411 - Winner: 0 - Epsilon: 0.11\n",
      "Episode 96/100 - Reward total: -8.00 - Loss: 0.4774 - Winner: 0 - Epsilon: 0.11\n",
      "Episode 97/100 - Reward total: -7.00 - Loss: 0.4623 - Winner: 0 - Epsilon: 0.11\n",
      "Episode 98/100 - Reward total: -8.00 - Loss: 0.4146 - Winner: 0 - Epsilon: 0.11\n",
      "Episode 99/100 - Reward total: -2.00 - Loss: 0.3685 - Winner: 0 - Epsilon: 0.11\n",
      "Episode 100/100 - Reward total: -6.00 - Loss: 0.3855 - Winner: 1 - Epsilon: 0.11\n",
      "Update target_net\n"
     ]
    }
   ],
   "source": [
    "list_reward, list_epsilon = train_dqn('test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_DQN(name, num_episodes_eval=50):\n",
    "    list_reward = []\n",
    "    win_count = 0\n",
    "    env = cathedral_v0.env(board_size=board_size, render_mode=\"text\", per_move_rewards=True, final_reward_score_difference=False)\n",
    "    env.reset()\n",
    "\n",
    "    n_actions = env.action_space(controlled_agent).n\n",
    "    obs_shape = env.observe(controlled_agent)[\"observation\"].shape  # (10, 10, 5)\n",
    "\n",
    "    if name != 'random':\n",
    "        checkpoint = torch.load(f\"model_weights_DQN/{name}.pth\", weights_only=False)\n",
    "\n",
    "        policy_net = DQN(obs_shape, n_actions).to(device)\n",
    "        policy_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        list_reward_training = checkpoint['list_reward']\n",
    "    \n",
    "    for episode in tqdm(range(num_episodes_eval)):\n",
    "        env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        while env.agents:\n",
    "            current_agent = env.agent_selection\n",
    "            observation = env.observe(current_agent)\n",
    "            legal_moves = [i for i, valid in enumerate(observation[\"action_mask\"]) if valid]\n",
    "            \n",
    "            if current_agent == controlled_agent:\n",
    "                state = observation[\"observation\"]\n",
    "                action_mask = observation[\"action_mask\"]\n",
    "                \n",
    "                if name == 'random':\n",
    "                    action = random.choice(legal_moves)\n",
    "                else:\n",
    "                    action = select_action_dqn(policy_net, state, action_mask, device)\n",
    "                \n",
    "                env.step(action)\n",
    "                reward = env.rewards[current_agent]\n",
    "                total_reward += reward\n",
    "\n",
    "            else:\n",
    "                action = random.choice(legal_moves)\n",
    "                env.step(action)\n",
    "        \n",
    "        list_reward.append(total_reward)\n",
    "        if env.winner == 0:  # controlled_agent (\"player_0\") gagne\n",
    "            win_count += 1\n",
    "        elif env.winner == -1:  # match nul : on peut compter 0.5 victoire\n",
    "            win_count += 0.5\n",
    "    \n",
    "    avg_reward = sum(list_reward)/len(list_reward)\n",
    "    print(f\"{num_episodes_eval} episodes => Avg Reward : {avg_reward} // Winrate : {win_count/num_episodes_eval}\")\n",
    "    env.close()\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_out_size : 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:21<00:00,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 episodes => Avg Reward : -0.14 // Winrate : 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "avg_reward = evaluate_DQN('test2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
