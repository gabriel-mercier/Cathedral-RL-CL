{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import sys\n",
                "sys.path.append('..')\n",
                "from cathedral_rl import cathedral_v0  \n",
                "from models.ppo_cnn import PPOCNN\n",
                "from ppo_utils import create_game_gif, RolloutBuffer, evaluate_ppo_against_random"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "################################## set device ##################################\n",
                "\n",
                "print(\"============================================================================================\")\n",
                "\n",
                "\n",
                "# set device to cpu or cuda\n",
                "device = torch.device('cpu')\n",
                "\n",
                "if(torch.cuda.is_available()): \n",
                "    device = torch.device('cuda:0') \n",
                "    torch.cuda.empty_cache()\n",
                "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
                "else:\n",
                "    print(\"Device set to : cpu\")\n",
                "    \n",
                "print(\"============================================================================================\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### PPO CNN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "K_epochs = 20               # update policy for K epochs\n",
                "eps_clip = 0.1             # clip parameter for PPO\n",
                "gamma = 0.97                # discount factor\n",
                "gae_lambda = 0.95\n",
                "\n",
                "lr_actor = 0.0005       # learning rate for actor network\n",
                "lr_critic = 0.0002\n",
                "\n",
                "board_size = 8\n",
                "num_episodes = 10000\n",
                "save_freq = 1000"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Pure self play of PPO vs itself"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_ppo_self_play(name):\n",
                "    env = cathedral_v0.env(board_size=board_size, render_mode=\"text\", per_move_rewards=True, final_reward_score_difference=False)\n",
                "    env.reset()\n",
                "    \n",
                "    gif_dir = os.path.join(\"game_recordings\", name)\n",
                "    os.makedirs(gif_dir, exist_ok=True)\n",
                "    \n",
                "    # Get action space and observation shape (should be the same for both players)\n",
                "    player_0 = \"player_0\"\n",
                "    player_1 = \"player_1\"\n",
                "    list_agents = env.agents\n",
                "    n_actions = env.action_space(player_0).n\n",
                "    obs_shape = env.observe(player_0)[\"observation\"].shape  # (10, 10, 5)\n",
                "    \n",
                "    episodes_per_update = 5\n",
                "    episode_counter = 0\n",
                "    \n",
                "    print(f'n_actions: {n_actions}')\n",
                "    print(f'observation shape: {obs_shape}')\n",
                "    \n",
                "    # Initialize primary PPO agent (the one that will be learning)\n",
                "    primary_agent = PPOCNN(\n",
                "        obs_shape=obs_shape,\n",
                "        action_dim=n_actions,\n",
                "        lr_actor=lr_actor,\n",
                "        lr_critic=lr_critic,\n",
                "        gamma=gamma,\n",
                "        K_epochs=K_epochs,\n",
                "        eps_clip=eps_clip\n",
                "    )\n",
                "    \n",
                "    # Initialize target PPO agent (the opponent that will be periodically updated)\n",
                "    target_agent = PPOCNN(\n",
                "        obs_shape=obs_shape,\n",
                "        action_dim=n_actions,\n",
                "        lr_actor=lr_actor,\n",
                "        lr_critic=lr_critic,\n",
                "        gamma=gamma,\n",
                "        K_epochs=K_epochs,\n",
                "        eps_clip=eps_clip\n",
                "    )\n",
                "    \n",
                "    # Clone the primary agent weights to target agent initially\n",
                "    target_agent.policy.load_state_dict(primary_agent.policy.state_dict())\n",
                "    \n",
                "    # Training statistics\n",
                "    list_reward_p0 = []\n",
                "    list_reward_p1 = []\n",
                "    policy_checkpoints = []\n",
                "    list_win_count_p0 = []\n",
                "    list_win_count_p1 = []\n",
                "    list_draw_count = []\n",
                "    surr1_array = []\n",
                "    surr2_array = []\n",
                "    values_array = []\n",
                "    mse_loss_array = []\n",
                "    total_loss_array = []\n",
                "    ratio_array = []\n",
                "    ratio_std_array = []\n",
                "    ppo_win_rates = []\n",
                "    random_win_rates = []\n",
                "    draw_rates = []\n",
                "    eval_episodes = []\n",
                "    \n",
                "    # Initialize tracking lists\n",
                "    if not list_win_count_p0:\n",
                "        list_win_count_p0 = [0]\n",
                "    if not list_win_count_p1:\n",
                "        list_win_count_p1 = [0]\n",
                "    if not list_draw_count:\n",
                "        list_draw_count = [0]\n",
                "    \n",
                "    # Training loop\n",
                "    for episode in range(num_episodes):\n",
                "        env.reset()\n",
                "        list_agents = env.agents.copy()\n",
                "        episode_buffer = RolloutBuffer()  # Create a fresh buffer for this episode\n",
                "        total_reward_p0 = 0\n",
                "        total_reward_p1 = 0\n",
                "        \n",
                "        # Game loop\n",
                "        while env.agents:\n",
                "            current_agent = env.agent_selection\n",
                "            observation = env.observe(current_agent)\n",
                "            state = observation[\"observation\"]\n",
                "            action_mask = observation[\"action_mask\"]\n",
                "            \n",
                "            # Player 0 (primary agent) is learning\n",
                "            if current_agent == player_0:\n",
                "                # Select action and store initial experience\n",
                "                action = primary_agent.select_action_episodic(state, action_mask, episode_buffer)\n",
                "            else:\n",
                "                # Target agent selects action (no exploration)\n",
                "                with torch.no_grad():\n",
                "                    action = target_agent.select_action_evaluation(state, action_mask)\n",
                "            \n",
                "            # Take action in environment\n",
                "            prev_observe = observation  # Store pre-action observation\n",
                "            env.step(action)\n",
                "            \n",
                "            # Get rewards\n",
                "            reward_p0 = env.rewards[\"player_0\"]\n",
                "            reward_p1 = env.rewards[\"player_1\"]\n",
                "            \n",
                "            # Check if game is over\n",
                "            game_over = (len(env.legal_moves[player_0]) == 0 and \n",
                "                         len(env.legal_moves[player_1]) == 0)\n",
                "            \n",
                "            # For primary agent (player 0) only, store experience\n",
                "            if current_agent == player_0:\n",
                "                # Store reward and terminal information\n",
                "                episode_buffer.rewards.append(reward_p0)\n",
                "                is_terminal = (current_agent not in env.agents) or game_over\n",
                "                episode_buffer.is_terminals.append(1 if is_terminal else 0)\n",
                "                \n",
                "                # Track total rewards\n",
                "                total_reward_p0 += reward_p0\n",
                "            else:\n",
                "                total_reward_p1 += reward_p1\n",
                "        \n",
                "        # After episode ends, add this episode's experiences to the main buffer\n",
                "        primary_agent.add_episode_buffer(episode_buffer)\n",
                "        episode_counter += 1\n",
                "        \n",
                "        # Update statistics\n",
                "        list_reward_p0.append(total_reward_p0)\n",
                "        list_reward_p1.append(total_reward_p1)\n",
                "        \n",
                "        # Update the appropriate counter based on the game outcome\n",
                "        winner = env.winner\n",
                "        \n",
                "        if list_agents[winner] == 'player_0':  # primary agent wins\n",
                "            list_win_count_p0.append(list_win_count_p0[-1] + 1)\n",
                "            list_win_count_p1.append(list_win_count_p1[-1])\n",
                "            list_draw_count.append(list_draw_count[-1])\n",
                "            \n",
                "        elif list_agents[winner] == 'player_1':  # target agent wins\n",
                "            list_win_count_p0.append(list_win_count_p0[-1])\n",
                "            list_win_count_p1.append(list_win_count_p1[-1] + 1)\n",
                "            list_draw_count.append(list_draw_count[-1])\n",
                "            \n",
                "        elif winner == -1:  # draw\n",
                "            list_win_count_p0.append(list_win_count_p0[-1])\n",
                "            list_win_count_p1.append(list_win_count_p1[-1])\n",
                "            list_draw_count.append(list_draw_count[-1])\n",
                "        \n",
                "        # Only update model after collecting experiences from multiple episodes\n",
                "        if episode_counter >= episodes_per_update:\n",
                "            surr_1, surr_2, value, mse_loss, total_loss, ratio, ratio_std = primary_agent.update()\n",
                "            # Reset episode counter after update\n",
                "            episode_counter = 0\n",
                "            \n",
                "            print(f'Ratio {ratio}, std {ratio_std}')\n",
                "            print(f'Surr1 = {surr_1}, surr2 {surr_2}, avg value {value}, mse_loss {mse_loss}, total_loss {total_loss}')\n",
                "            # Record metrics only when updates happen\n",
                "            surr1_array.append(surr_1)\n",
                "            surr2_array.append(surr_2)\n",
                "            values_array.append(value)\n",
                "            mse_loss_array.append(mse_loss)\n",
                "            total_loss_array.append(total_loss)\n",
                "            ratio_array.append(ratio)\n",
                "            ratio_std_array.append(ratio_std)\n",
                "        \n",
                "        # Update target network with soft updates more frequently\n",
                "        if (episode + 1) % 10 == 0:\n",
                "            # Soft update (interpolate between models)\n",
                "            for target_param, param in zip(target_agent.policy.parameters(), \n",
                "                                          primary_agent.policy.parameters()):\n",
                "                target_param.data.copy_(\n",
                "                    0.7 * target_param.data + 0.3 * param.data\n",
                "                )\n",
                "                \n",
                "            target_agent.policy_old.load_state_dict(target_agent.policy.state_dict())\n",
                "        if (episode + 1) % 5 == 0:\n",
                "            win_rate_p0 = list_win_count_p0[-1] / (episode + 1)\n",
                "            win_rate_p1 = list_win_count_p1[-1] / (episode + 1)\n",
                "            draw_rate = list_draw_count[-1] / (episode + 1)\n",
                "            \n",
                "            print(f\"Episode {episode+1}/{num_episodes}\")\n",
                "            print(f\"  Primary Agent (P0) - Avg Reward: {sum(list_reward_p0[-100:]) / min(100, len(list_reward_p0)):.2f} - Win Rate: {win_rate_p0:.2f}\")\n",
                "            print(f\"  Target Agent (P1) - Avg Reward: {sum(list_reward_p1[-100:]) / min(100, len(list_reward_p1)):.2f} - Win Rate: {win_rate_p1:.2f}\")\n",
                "            print(f\"  Draw Rate: {draw_rate:.2f}\")\n",
                "\n",
                "        if (episode + 1) % 100 == 0:\n",
                "            # Complete copy of primary agent\n",
                "            target_agent.policy.load_state_dict(primary_agent.policy.state_dict())\n",
                "            target_agent.policy_old.load_state_dict(target_agent.policy.state_dict())\n",
                "            print(\"Target agent fully synchronized with primary agent\")\n",
                " \n",
                "        if(episode + 1) % 300 == 0:\n",
                "            buffer_states = primary_agent.buffer.states.copy() if hasattr(primary_agent.buffer.states, 'copy') else primary_agent.buffer.states[:]\n",
                "            buffer_actions = primary_agent.buffer.actions.copy() if hasattr(primary_agent.buffer.actions, 'copy') else primary_agent.buffer.actions[:]\n",
                "            buffer_logprobs = primary_agent.buffer.logprobs.copy() if hasattr(primary_agent.buffer.logprobs, 'copy') else primary_agent.buffer.logprobs[:]\n",
                "            buffer_state_values = primary_agent.buffer.state_values.copy() if hasattr(primary_agent.buffer.state_values, 'copy') else primary_agent.buffer.state_values[:]\n",
                "            buffer_action_masks = primary_agent.buffer.action_masks.copy() if hasattr(primary_agent.buffer.action_masks, 'copy') else primary_agent.buffer.action_masks[:]\n",
                "            buffer_rewards = primary_agent.buffer.rewards.copy() if hasattr(primary_agent.buffer.rewards, 'copy') else primary_agent.buffer.rewards[:]\n",
                "            buffer_is_terminals = primary_agent.buffer.is_terminals.copy() if hasattr(primary_agent.buffer.is_terminals, 'copy') else primary_agent.buffer.is_terminals[:]\n",
                "            \n",
                "            # Create the GIF\n",
                "            create_game_gif(episode, primary_agent, gif_dir, board_size)\n",
                "            print(f\"Created GIF for episode {episode+1}\")\n",
                "            \n",
                "            # Restore buffer state\n",
                "            primary_agent.buffer.clear()\n",
                "            primary_agent.buffer.states = buffer_states\n",
                "            primary_agent.buffer.actions = buffer_actions\n",
                "            primary_agent.buffer.logprobs = buffer_logprobs\n",
                "            primary_agent.buffer.state_values = buffer_state_values\n",
                "            primary_agent.buffer.action_masks = buffer_action_masks\n",
                "            primary_agent.buffer.rewards = buffer_rewards\n",
                "            primary_agent.buffer.is_terminals = buffer_is_terminals\n",
                "            \n",
                "            stats = evaluate_ppo_against_random(primary_agent, 50, 8)\n",
                "            ppo_win_rate = stats['ppo_win_rate']\n",
                "            random_win_rate = stats['random_win_rate']\n",
                "            draw_rate = stats['draw_rate']\n",
                "            ppo_win_rates.append(ppo_win_rate)\n",
                "            random_win_rates.append(random_win_rate)\n",
                "            draw_rates.append(draw_rate)\n",
                "            eval_episodes.append(episode + 1)\n",
                "            \n",
                "            \n",
                "        # Save checkpoint\n",
                "        if (episode+1) % save_freq == 0:\n",
                "            policy_checkpoints.append(primary_agent.policy.state_dict())\n",
                "            print(\"Checkpoint saved\")\n",
                "            \n",
                "            # Save model\n",
                "            os.makedirs(\"model_weights_adversarial_PPO\", exist_ok=True)\n",
                "            primary_agent.save(f\"model_weights_adversarial_PPO/{name}_{episode+1}.pth\")\n",
                "    \n",
                "    # Save final model and training statistics\n",
                "    os.makedirs(\"model_weights_adversarial_PPO\", exist_ok=True)\n",
                "    print(f'Saving final model')\n",
                "    torch.save({\n",
                "        'model_state_dict': primary_agent.policy.state_dict(),\n",
                "        'list_reward_p0': list_reward_p0,\n",
                "        'list_reward_p1': list_reward_p1,\n",
                "        'list_win_count_p0': list_win_count_p0,\n",
                "        'list_win_count_p1': list_win_count_p1, \n",
                "        'list_draw_count': list_draw_count,\n",
                "        'policy_checkpoints': policy_checkpoints,\n",
                "        'num_checkpoints': len(policy_checkpoints)\n",
                "    }, f\"model_weights_adversarial_PPO/{name}_final.pth\")\n",
                "    \n",
                "    env.close()\n",
                "    \n",
                "    return list_reward_p0, list_reward_p1, list_win_count_p0, list_win_count_p1, list_draw_count, surr1_array, surr2_array, values_array, mse_loss_array, total_loss_array, ratio_array, ratio_std_array, ppo_win_rates, random_win_rates, draw_rates, eval_episodes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rewards_p0, rewards_p1, win_counts_p0, win_counts_p1, draw_counts,  surr1_array, surr2_array, values_array, mse_loss_array, total_loss_array,ratio_array, ratio_std_array, ppo_win_rates, random_win_rates, eval_draw_rates, eval_episodes = train_ppo_self_play(\"cathedral_ppo_self_play_adversarial\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Plot results\n",
                "plt.figure(figsize=(15, 15))\n",
                "\n",
                "# Plot rewards for both players\n",
                "plt.subplot(4, 2, 1)\n",
                "plt.plot(rewards_p0, label='Player 0', alpha=0.7)\n",
                "plt.plot(rewards_p1, label='Player 1', alpha=0.7)\n",
                "plt.title(\"Rewards per Episode\")\n",
                "plt.xlabel(\"Episode\")\n",
                "plt.ylabel(\"Total Reward\")\n",
                "plt.legend()\n",
                "\n",
                "# Plot moving average of rewards for better visualization\n",
                "window = 100\n",
                "plt.subplot(4, 2, 2)\n",
                "p0_avg = [np.mean(rewards_p0[max(0, i-window):i+1]) for i in range(len(rewards_p0))]\n",
                "p1_avg = [np.mean(rewards_p1[max(0, i-window):i+1]) for i in range(len(rewards_p1))]\n",
                "plt.plot(p0_avg, label='Player 0 (100-ep avg)', alpha=0.7)\n",
                "plt.plot(p1_avg, label='Player 1 (100-ep avg)', alpha=0.7)\n",
                "plt.title(\"Moving Average Rewards (100 episodes)\")\n",
                "plt.xlabel(\"Episode\")\n",
                "plt.ylabel(\"Average Reward\")\n",
                "plt.legend()\n",
                "\n",
                "# Plot win rates for both players\n",
                "plt.subplot(4, 2, 3)\n",
                "total_games = np.arange(1, len(win_counts_p0) + 1)\n",
                "p0_win_rates = np.array(win_counts_p0) / total_games\n",
                "p1_win_rates = np.array(win_counts_p1) / total_games\n",
                "draw_rates = np.array(draw_counts) / total_games\n",
                "\n",
                "plt.plot(total_games, p0_win_rates, label='Player 0 Win Rate', color='blue')\n",
                "plt.plot(total_games, p1_win_rates, label='Player 1 Win Rate', color='orange')\n",
                "plt.plot(total_games, draw_rates, label='Draw Rate', color='green')\n",
                "plt.title(\"Cumulative Win/Draw Rates\")\n",
                "plt.xlabel(\"Episode\")\n",
                "plt.ylabel(\"Rate\")\n",
                "plt.ylim(0, 1)\n",
                "plt.legend()\n",
                "plt.grid(True, linestyle='--', alpha=0.5)\n",
                "\n",
                "# Plot surrogate losses\n",
                "plt.subplot(4, 2, 4)\n",
                "plt.plot(surr1_array, label='Surrogate Loss 1', alpha=0.7)\n",
                "plt.plot(surr2_array, label='Surrogate Loss 2', alpha=0.7)\n",
                "plt.title(\"Surrogate Losses\")\n",
                "plt.xlabel(\"Training Step\")\n",
                "plt.ylabel(\"Loss\")\n",
                "plt.legend()\n",
                "\n",
                "# Plot value function loss\n",
                "plt.subplot(4, 2, 5)\n",
                "plt.plot(values_array, label='Value Function', color='purple')\n",
                "plt.title(\"Value Function Over Training\")\n",
                "plt.xlabel(\"Training Step\")\n",
                "plt.ylabel(\"Value\")\n",
                "plt.legend()\n",
                "\n",
                "# Plot loss components\n",
                "plt.subplot(4, 2, 6)\n",
                "plt.plot(mse_loss_array, label='MSE Loss', alpha=0.7)\n",
                "plt.plot(total_loss_array, label='Total Loss', alpha=0.7)\n",
                "plt.title(\"Loss Metrics\")\n",
                "plt.xlabel(\"Training Step\")\n",
                "plt.ylabel(\"Loss\")\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(4, 2, 7)\n",
                "plt.plot(ratio_array, label='Ratio', alpha=0.7, color='blue')\n",
                "plt.fill_between(range(len(ratio_array)), \n",
                "                np.array(ratio_array) - np.array(ratio_std_array), \n",
                "                np.array(ratio_array) + np.array(ratio_std_array), \n",
                "                color='blue', alpha=0.3, label='Ratio Std Dev')\n",
                "plt.title(\"Ratio and Standard Deviation\")\n",
                "plt.xlabel(\"Training Step\")\n",
                "plt.ylabel(\"Ratio\")\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(4, 2, 8)\n",
                "plt.plot(eval_episodes, ppo_win_rates, label='PPO Win Rate', color='purple', marker='o')\n",
                "plt.plot(eval_episodes, random_win_rates, label='Random Win Rate', color='red', marker='s')\n",
                "plt.plot(eval_episodes, eval_draw_rates, label='Draw Rate', color='gray', marker='^')\n",
                "plt.title(\"Performance vs Random Agent\")\n",
                "plt.xlabel(\"Episode\")\n",
                "plt.ylabel(\"Win Rate\")\n",
                "plt.ylim(0, 1)\n",
                "plt.grid(True, linestyle='--', alpha=0.5)\n",
                "plt.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(\"ppo_self_play_training_results_extended.png\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_ppo_against_random(model_path, num_episodes=100, board_size=8, render=False):\n",
                "    \"\"\"\n",
                "    Evaluate a trained PPO agent against a random policy after training is complete.\n",
                "    \n",
                "    Args:\n",
                "        model_path (str): Path to the saved PPO model weights\n",
                "        num_episodes (int): Number of evaluation episodes\n",
                "        board_size (int): Size of the board\n",
                "        render (bool): Whether to render the game\n",
                "        \n",
                "    Returns:\n",
                "        dict: Evaluation metrics including win rate\n",
                "    \"\"\"\n",
                "    env = cathedral_v0.env(\n",
                "        board_size=board_size, \n",
                "        render_mode=\"ansi\" if render else None, \n",
                "        per_move_rewards=True, \n",
                "        final_reward_score_difference=False\n",
                "    )\n",
                "    env.reset()\n",
                "    \n",
                "    # Get observation shape and action space\n",
                "    player_0 = \"player_0\"  # PPO agent\n",
                "    player_1 = \"player_1\"  # Random agent\n",
                "    n_actions = env.action_space(player_0).n\n",
                "    obs_shape = env.observe(player_0)[\"observation\"].shape\n",
                "    \n",
                "    print(f\"Loading model from {model_path}\")\n",
                "    \n",
                "    # Initialize PPO agent\n",
                "    ppo_agent = PPOCNN(\n",
                "        obs_shape=obs_shape,\n",
                "        action_dim=n_actions,\n",
                "        lr_actor=lr_actor, \n",
                "        lr_critic=lr_critic,\n",
                "        gamma=gamma,\n",
                "        K_epochs=K_epochs,\n",
                "        eps_clip=eps_clip\n",
                "    )\n",
                "    \n",
                "    # Load model weights\n",
                "    checkpoint = torch.load(model_path, weights_only=False)\n",
                "    ppo_agent.policy.load_state_dict(checkpoint)\n",
                "    ppo_agent.policy_old.load_state_dict(ppo_agent.policy.state_dict())\n",
                "    \n",
                "    # Set to evaluation mode\n",
                "    ppo_agent.policy.eval()\n",
                "    \n",
                "    # Statistics\n",
                "    stats = {\n",
                "        'ppo_wins': 0,\n",
                "        'random_wins': 0,\n",
                "        'draws': 0,\n",
                "        'ppo_rewards': [],\n",
                "        'random_rewards': [],\n",
                "    }\n",
                "    \n",
                "    for episode in range(num_episodes):\n",
                "        env.reset()\n",
                "        list_agents = env.agents\n",
                "        episode_reward_ppo = 0\n",
                "        episode_reward_random = 0\n",
                "        \n",
                "        # Print progress\n",
                "        if episode % 10 == 0:\n",
                "            print(f\"Playing episode {episode}/{num_episodes}\")\n",
                "        \n",
                "        # Clear buffer for evaluation (since we don't need to preserve it post-training)\n",
                "        ppo_agent.buffer.clear()\n",
                "        \n",
                "        while env.agents:\n",
                "            current_agent = env.agent_selection\n",
                "            observation = env.observe(current_agent)\n",
                "            \n",
                "            if current_agent == player_0:  # PPO agent's turn\n",
                "                state = observation[\"observation\"]\n",
                "                action_mask = observation[\"action_mask\"]\n",
                "                \n",
                "                # Use PPO to select action (deterministic for evaluation)\n",
                "                action = ppo_agent.select_action_evaluation(state, action_mask)\n",
                "                \n",
                "                if render:\n",
                "                    print(f'PPO action {action}')\n",
                "                \n",
                "            else:  # Random agent's turn\n",
                "                action_mask = observation[\"action_mask\"]\n",
                "                \n",
                "                # Random valid action\n",
                "                valid_actions = np.where(action_mask == 1)[0]\n",
                "                action = np.random.choice(valid_actions)\n",
                "                \n",
                "                if render:\n",
                "                    print(f'Random action {action}')\n",
                "            \n",
                "            # Take action\n",
                "            env.step(action)\n",
                "            \n",
                "            # Track rewards\n",
                "            if current_agent == player_0:\n",
                "                episode_reward_ppo += env.rewards[current_agent]\n",
                "            else:\n",
                "                episode_reward_random += env.rewards[current_agent]\n",
                "        \n",
                "        # Record game outcome\n",
                "        winner = env.winner\n",
                "        if winner != -1 and list_agents[winner] == player_0:\n",
                "            stats['ppo_wins'] += 1\n",
                "            if render:\n",
                "                print(f'Episode {episode} - PPO wins!')\n",
                "        elif winner != -1 and list_agents[winner] == player_1:\n",
                "            stats['random_wins'] += 1\n",
                "            if render:\n",
                "                print(f'Episode {episode} - Random wins!')\n",
                "        else:  # Draw\n",
                "            stats['draws'] += 1\n",
                "            if render:\n",
                "                print(f'Episode {episode} - Draw!')\n",
                "        \n",
                "        # Record episode rewards\n",
                "        stats['ppo_rewards'].append(episode_reward_ppo)\n",
                "        stats['random_rewards'].append(episode_reward_random)\n",
                "    \n",
                "    # Calculate win rates\n",
                "    stats['ppo_win_rate'] = stats['ppo_wins'] / num_episodes\n",
                "    stats['random_win_rate'] = stats['random_wins'] / num_episodes\n",
                "    stats['draw_rate'] = stats['draws'] / num_episodes\n",
                "    \n",
                "    # Calculate average rewards\n",
                "    stats['avg_ppo_reward'] = sum(stats['ppo_rewards']) / num_episodes\n",
                "    stats['avg_random_reward'] = sum(stats['random_rewards']) / num_episodes\n",
                "    \n",
                "    # Print summary\n",
                "    print(\"\\n===== Evaluation Results =====\")\n",
                "    print(f\"Episodes played: {num_episodes}\")\n",
                "    print(f\"PPO Wins: {stats['ppo_wins']} ({stats['ppo_win_rate']:.2%})\")\n",
                "    print(f\"Random Wins: {stats['random_wins']} ({stats['random_win_rate']:.2%})\")\n",
                "    print(f\"Draws: {stats['draws']} ({stats['draw_rate']:.2%})\")\n",
                "    print(f\"Average PPO Reward: {stats['avg_ppo_reward']:.2f}\")\n",
                "    print(f\"Average Random Reward: {stats['avg_random_reward']:.2f}\")\n",
                "    \n",
                "    env.close()\n",
                "    return stats"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_path = \"model_weights_adversarial_PPO/cathedral_ppo_self_play_adversarial_3000.pth\"\n",
                "\n",
                "eval_stats = evaluate_ppo_against_random(\n",
                "    model_path=model_path,\n",
                "    num_episodes=200,\n",
                "    board_size=8,\n",
                "    render=False\n",
                ")\n",
                "\n",
                "# Create a bar chart of win rates\n",
                "labels = ['PPO Agent', 'Random Agent', 'Draw']\n",
                "values = [eval_stats['ppo_win_rate'], eval_stats['random_win_rate'], eval_stats['draw_rate']]\n",
                "colors = ['green', 'red', 'gray']\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.bar(labels, values, color=colors)\n",
                "plt.title('Win Rate Comparison')\n",
                "plt.ylabel('Win Rate')\n",
                "plt.ylim(0, 1)\n",
                "\n",
                "for i, v in enumerate(values):\n",
                "    plt.text(i, v + 0.02, f'{v:.2%}', ha='center')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('ppo_vs_random_evaluation.png')\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "cathedral",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
