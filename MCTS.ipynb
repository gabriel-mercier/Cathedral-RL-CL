{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cathedral_rl import cathedral_v0  \n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import copy\n",
    "from cathedral_rl.game.board import Board \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaZeroNet(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions):\n",
    "        super(AlphaZeroNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(obs_shape[2], 32, kernel_size=3, stride=1, padding=1),  # -> 32 x 10 x 10\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),              # -> 64 x 10 x 10\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        dummy = torch.zeros(1, obs_shape[2], obs_shape[0], obs_shape[1])\n",
    "        conv_out_size = self.conv(dummy).shape[1]\n",
    "        \n",
    "        self.policy_fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "        \n",
    "        self.value_fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Tanh()  \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        features = self.conv(x)\n",
    "        policy_logits = self.policy_fc(features)\n",
    "        value = self.value_fc(features)\n",
    "        return policy_logits, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_state(board_size=10):\n",
    "    board = Board(board_size=board_size)\n",
    "    # \"player_0\" starts\n",
    "    return {\"board\": board, \"current_agent\": \"player_0\"}\n",
    "\n",
    "def get_legal_moves(state):\n",
    "    board = state[\"board\"]\n",
    "    agent = state[\"current_agent\"]\n",
    "    legal_moves = []\n",
    "    for action in range(board.num_actions):\n",
    "        if board.is_legal(agent, action):\n",
    "            legal_moves.append(action)\n",
    "    return legal_moves\n",
    "\n",
    "def is_terminal(state):\n",
    "    \"\"\"\n",
    "    Determine is the state is terminal.\n",
    "    A state is terminal if no legal moves are available for both players.\n",
    "    \"\"\"\n",
    "    board = state[\"board\"]\n",
    "    current_agent = state[\"current_agent\"]\n",
    "    legal_moves_current = [action for action in range(board.num_actions) if board.is_legal(current_agent, action)]\n",
    "    \n",
    "    opponent = \"player_1\" if current_agent == \"player_0\" else \"player_0\"\n",
    "    legal_moves_opponent = [action for action in range(board.num_actions) if board.is_legal(opponent, action)]\n",
    "    \n",
    "    return (len(legal_moves_current) == 0) and (len(legal_moves_opponent) == 0)\n",
    "\n",
    "def evaluate_terminal(state):\n",
    "    \"\"\"\n",
    "    Evaluate a terminal state by returning +1, -1 or 0.\n",
    "    We use the board.check_for_winner() function which returns:\n",
    "        - 0 if player_0 wins,\n",
    "        - 1 if player_1 wins,\n",
    "        - -1 in case of a draw.\n",
    "    The result is returned from the point of view of the agent that was active in the initial state of the self-play.\n",
    "    \"\"\"\n",
    "    board = state[\"board\"]\n",
    "    winner, _, _ = board.check_for_winner()\n",
    "    current_agent = state[\"current_agent\"]\n",
    "    if winner == -1:\n",
    "        return 0  # draw\n",
    "    if (winner == 0 and current_agent == \"player_0\") or (winner == 1 and current_agent == \"player_1\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def next_state(state, action):\n",
    "    \"\"\"\n",
    "    From a state and an action, simulates the move and returns the new state.\n",
    "    Here we perform a deep copy of the state to avoid altering the original.\n",
    "    \"\"\"\n",
    "    new_state = copy.deepcopy(state)\n",
    "    board = new_state[\"board\"]\n",
    "    agent = new_state[\"current_agent\"]\n",
    "    \n",
    "    board.play_turn(agent, action)\n",
    "\n",
    "    opponent = \"player_1\" if agent == \"player_0\" else \"player_0\"\n",
    "    legal_moves_opponent = [a for a in range(board.num_actions) if board.is_legal(opponent, a)]\n",
    "    if len(legal_moves_opponent) > 0:\n",
    "        new_state[\"current_agent\"] = opponent\n",
    "    else:\n",
    "        # If the opponent has no legal moves, the current agent plays again\n",
    "        new_state[\"current_agent\"] = agent\n",
    "        \n",
    "    return new_state\n",
    "\n",
    "def state_to_observation(state):\n",
    "    \"\"\"\n",
    "    Converts the state (dictionary containing \"board\" and \"current_agent\")\n",
    "    into an observation (numpy array of shape (board_size, board_size, 5)).\n",
    "    Inspired by the observe function of the environment.\n",
    "    \"\"\"\n",
    "    board = state[\"board\"]\n",
    "    agent = state[\"current_agent\"]\n",
    "    board_size = board.board_size\n",
    "\n",
    "    board_vals = board.squares.reshape(board_size, board_size)\n",
    "    board_territory = board.territory.reshape(board_size, board_size)\n",
    "    \n",
    "    cur_player = board.possible_agents.index(agent)\n",
    "    opp_player = (cur_player + 1) % 2\n",
    "\n",
    "    cur_p_board = np.equal(board_vals, cur_player + 1)\n",
    "    opp_p_board = np.equal(board_vals, opp_player + 1)\n",
    "    cathedral_board = np.equal(board_vals, 3)\n",
    "    cur_p_territory = np.equal(board_territory, cur_player + 1)\n",
    "    opp_p_territory = np.equal(board_territory, opp_player + 1)\n",
    "\n",
    "    observation = np.stack(\n",
    "        [cur_p_board, opp_p_board, cathedral_board, cur_p_territory, opp_p_territory],\n",
    "        axis=2\n",
    "    ).astype(np.float32)\n",
    "    return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSNode:\n",
    "    def __init__(self, state, parent=None, prior=0.0):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.children = {}  # dictionnaire : action -> MCTSNode\n",
    "        self.N = 0          # number of visits\n",
    "        self.W = 0.0        # sum of values\n",
    "        self.Q = 0.0        # mean value\n",
    "        self.P = prior      # prior probability from the policy network\n",
    "        self.is_expanded = False\n",
    "\n",
    "def select_child(node, c_puct):\n",
    "    best_score = -float('inf')\n",
    "    best_action = None\n",
    "    best_child = None\n",
    "    count = 0\n",
    "    print(f'total child : {len(node.children)}')\n",
    "    for action, child in node.children.items():\n",
    "        # print(f'count select child: {count}')\n",
    "        count += 1\n",
    "        U = c_puct * child.P * math.sqrt(node.N) / (1 + child.N)\n",
    "        score = child.Q + U\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_action = action\n",
    "            best_child = child\n",
    "    return best_action, best_child\n",
    "\n",
    "def expand_node(node, network, device):\n",
    "    legal_moves = get_legal_moves(node.state)\n",
    "    print(f'legal moves : {len(legal_moves)}')\n",
    "    \n",
    "    obs = state_to_observation(node.state)\n",
    "    print(f'obs shape : {obs.shape}')\n",
    "    state_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    policy_logits, value = network(state_tensor)\n",
    "    \n",
    "    policy = F.softmax(policy_logits.squeeze(0), dim=0).detach().cpu().numpy()\n",
    "    \n",
    "    print(f'policy shape : {policy.shape}')\n",
    "    print(f'policy : {policy}')\n",
    "    count = 0\n",
    "    for action in legal_moves:\n",
    "        count += 1\n",
    "        if action not in node.children:\n",
    "            new_state = next_state(node.state, action)\n",
    "            node.children[action] = MCTSNode(new_state, parent=node, prior=policy[action])\n",
    "    node.is_expanded = True\n",
    "    return value.item()\n",
    "\n",
    "\n",
    "def mcts_search(root, network, device, n_simulations, c_puct, n_actions):\n",
    "    for _ in range(n_simulations):\n",
    "        print(f'simulation: {_}')\n",
    "        node = root\n",
    "        search_path = [node]\n",
    "        count = 0\n",
    "        while node.is_expanded and node.children:\n",
    "            print(f'count expanding: {count}')\n",
    "            count += 1\n",
    "            action, node = select_child(node, c_puct)\n",
    "            search_path.append(node)\n",
    "            \n",
    "        print(\"test if terminal\")\n",
    "        if is_terminal(node.state):\n",
    "            print(\"IS TERMINAL\")\n",
    "            value = evaluate_terminal(node.state)\n",
    "        else:\n",
    "            print(\"EXPANDING\")\n",
    "            value = expand_node(node, network, device, n_actions)\n",
    "            \n",
    "        # Backpropagation\n",
    "        print(\"backpropagating\")\n",
    "        for i in reversed(range(len(search_path))):\n",
    "            node = search_path[i]\n",
    "            node.N += 1\n",
    "            node.W += value\n",
    "            node.Q = node.W / node.N\n",
    "            if i > 0:\n",
    "                parent_agent = search_path[i - 1].state[\"current_agent\"]\n",
    "                current_agent = node.state[\"current_agent\"]\n",
    "                if parent_agent != current_agent:\n",
    "                    value = -value\n",
    "\n",
    "\n",
    "\n",
    "def get_policy_from_mcts(root, n_actions, temperature):\n",
    "    counts = np.zeros(n_actions)\n",
    "    for action, child in tqdm(root.children.items()):\n",
    "        counts[action] = child.N\n",
    "    if temperature == 0:\n",
    "        # Deterministic policy\n",
    "        best_action = np.argmax(counts)\n",
    "        policy = np.zeros_like(counts)\n",
    "        policy[best_action] = 1.0\n",
    "    else:\n",
    "        counts_temp = counts ** (1 / temperature)\n",
    "        policy = counts_temp / np.sum(counts_temp)\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_play_game(network, device, n_simulations, c_puct, temperature, n_actions):\n",
    "    game_history = []  # (state, pi, joueur)\n",
    "    state = initial_state(board_size=10)\n",
    "    current_player = 1  # par exemple, 1 pour le joueur courant, -1 pour l'adversaire\n",
    "    while not is_terminal(state):\n",
    "        print(\"Creating root node\")\n",
    "        root = MCTSNode(state)\n",
    "        \n",
    "        print(\"Running MCTS\")\n",
    "        mcts_search(root, network, device, n_simulations, c_puct, n_actions)\n",
    "        \n",
    "        print(\"Extracting policy\")\n",
    "        pi = get_policy_from_mcts(root, n_actions, temperature)\n",
    "        \n",
    "        game_history.append((state, pi, current_player))\n",
    "        \n",
    "        action = np.random.choice(n_actions, p=pi)\n",
    "        \n",
    "        state = next_state(state, action)\n",
    "        current_player = -current_player\n",
    "    \n",
    "    outcome = evaluate_terminal(state) \n",
    "    training_examples = []\n",
    "    for s, pi, player in game_history:\n",
    "        value_target = outcome if player == 1 else -outcome\n",
    "        training_examples.append((s, pi, value_target))\n",
    "    return training_examples\n",
    "\n",
    "def train_alphazero(network, optimizer, device, num_iterations, n_games, n_simulations, c_puct, temperature, n_actions, batch_size):\n",
    "    memory = []\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f'iteration: {iteration}')\n",
    "        # Collecte d'exemples via self-play\n",
    "        for game in range(n_games):\n",
    "            print(f'game: {game}')\n",
    "            game_data = self_play_game(network, device, n_simulations, c_puct, temperature, n_actions)\n",
    "            memory.extend(game_data)\n",
    "        \n",
    "        # Mélanger et échantillonner un batch pour l'entraînement\n",
    "        batch = random.sample(memory, batch_size)\n",
    "        states, target_policies, target_values = zip(*batch)\n",
    "        \n",
    "        states_tensor = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
    "        target_policies_tensor = torch.tensor(np.array(target_policies), dtype=torch.float32).to(device)\n",
    "        target_values_tensor = torch.tensor(target_values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        \n",
    "        # Passage dans le réseau\n",
    "        pred_policies_logits, pred_values = network(states_tensor)\n",
    "        # Calcul de la loss de valeur (MSE)\n",
    "        value_loss = F.mse_loss(pred_values, target_values_tensor)\n",
    "        # Calcul de la loss de politique (cross-entropy, on prend le log softmax sur les logits)\n",
    "        policy_loss = -torch.mean(torch.sum(target_policies_tensor * F.log_softmax(pred_policies_logits, dim=1), dim=1))\n",
    "        loss = value_loss + policy_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Iteration {iteration+1}/{num_iterations}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation :\n",
    "\n",
    "# Paramètres du jeu et du réseau\n",
    "env = cathedral_v0.env(board_size=10, render_mode=\"text\", per_move_rewards=False, final_reward_score_difference=False)\n",
    "env.reset()\n",
    "n_actions = env.action_space(\"player_0\" ).n\n",
    "\n",
    "obs_shape = (10, 10, 5)         # nombre total d'actions possibles (à définir selon votre jeu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instanciation du réseau et de l'optimiseur\n",
    "network = AlphaZeroNet(obs_shape, n_actions).to(device)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "# Paramètres du MCTS et de l'entraînement\n",
    "n_simulations = 5           # nombre de simulations MCTS par coup\n",
    "c_puct = 1.0                 # coefficient d'exploration pour le MCTS\n",
    "temperature = 1.0            # contrôle l'exploration lors du choix d'action\n",
    "num_iterations = 2      # nombre d'itérations d'entraînement\n",
    "n_games = 2                 # nombre de parties auto-jouées par itération\n",
    "batch_size = 8              # taille du batch pour l'entraînement\n",
    "\n",
    "# Lancement de l'entraînement\n",
    "train_alphazero(network, optimizer, device, num_iterations, n_games, n_simulations, c_puct, temperature, n_actions, batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
