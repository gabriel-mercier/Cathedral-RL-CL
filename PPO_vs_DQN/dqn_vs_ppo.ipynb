{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DQN.model import DQN\n",
    "from DQN.utils import select_action_dqn\n",
    "from PPO.models.ppo_cnn import PPOCNN\n",
    "from cathedral_rl import cathedral_v0\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import pygame\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_epochs = 40               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.95                # discount factor\n",
    "gae_lambda = 0.95\n",
    "\n",
    "lr_actor = 0.005       # learning rate for actor network\n",
    "lr_critic = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_vs_ppo(model_dqn, model_ppo, board_size, device, num_episodes_eval=1000, epsilon=0.15):\n",
    "    env = cathedral_v0.env(board_size=board_size, render_mode=\"text\", per_move_rewards=True, final_reward_score_difference=False)\n",
    "    env.reset()\n",
    "    \n",
    "    player_0='player_0' # player_0 or player_1 (does not matter)\n",
    "    player_1='player_1'\n",
    "    n_actions = env.action_space(player_1).n\n",
    "    obs_shape = env.observe(player_1)[\"observation\"].shape \n",
    "    \n",
    "    dqn_net = DQN(obs_shape, n_actions).to(device)\n",
    "\n",
    "    checkpoint_dqn = torch.load(f\"models/{model_dqn}.pth\", weights_only=False)\n",
    "    dqn_net.load_state_dict(checkpoint_dqn['model_state_dict'])\n",
    "\n",
    "\n",
    "    checkpoint_ppo = torch.load(f\"models/{model_ppo}.pth\", weights_only=False)\n",
    "    ppo_agent = PPOCNN(\n",
    "        obs_shape=obs_shape,\n",
    "        action_dim=n_actions,\n",
    "        lr_actor=lr_actor, \n",
    "        lr_critic=lr_critic,\n",
    "        gamma=gamma,\n",
    "        K_epochs=K_epochs,\n",
    "        eps_clip=eps_clip\n",
    "    )\n",
    "    if 'model_state_dict' in checkpoint_ppo:\n",
    "        ppo_agent.policy.load_state_dict(checkpoint_ppo['model_state_dict'])\n",
    "    else:\n",
    "        # Direct state dict (from .save() method)\n",
    "        ppo_agent.policy.load_state_dict(checkpoint_ppo)\n",
    "    \n",
    "    list_reward_dqn = []\n",
    "    win_count_dqn = 0\n",
    "    list_reward_ppo = []\n",
    "    win_count_ppo = 0\n",
    "    win_count_draw = 0\n",
    "        \n",
    "    for episode in tqdm(range(num_episodes_eval)):\n",
    "        env.reset()\n",
    "        list_agents = env.agents\n",
    "\n",
    "        while env.agents:\n",
    "            current_agent = env.agent_selection\n",
    "            observation = env.observe(current_agent)\n",
    "            legal_moves = [i for i, valid in enumerate(observation[\"action_mask\"]) if valid]\n",
    "            state = observation[\"observation\"]\n",
    "            action_mask = observation[\"action_mask\"]\n",
    "\n",
    "            if current_agent == player_0: # dqn plays\n",
    "                _, action = select_action_dqn(dqn_net, state, action_mask, legal_moves, device, 'boltzmann', 0, 0.01)\n",
    "                reward = env.rewards[current_agent]\n",
    "                list_reward_dqn.append(reward)\n",
    "                \n",
    "            else:  \n",
    "                with torch.no_grad():\n",
    "                    action = ppo_agent.select_action_evaluation(state, action_mask)\n",
    "                reward = env.rewards[current_agent]\n",
    "                list_reward_ppo.append(reward)\n",
    "            \n",
    "            env.step(action)\n",
    "                        \n",
    "        winner = env.winner\n",
    "        if winner == -1:\n",
    "            win_count_draw += 1\n",
    "        else:\n",
    "            if list_agents[winner] == player_0:  # dqn wins\n",
    "                win_count_dqn += 1\n",
    "            else:\n",
    "                win_count_ppo += 1\n",
    "\n",
    "        \n",
    "    avg_reward_dqn = sum(list_reward_dqn)/len(list_reward_dqn)\n",
    "    avg_reward_ppo = sum(list_reward_ppo)/len(list_reward_ppo)\n",
    "    winrate_dqn = win_count_dqn/num_episodes_eval\n",
    "    winrate_ppo = win_count_ppo/num_episodes_eval\n",
    "    winrate_draw = win_count_draw/num_episodes_eval\n",
    "    print(f\" {num_episodes_eval} episodes:\\nAvg Reward DQN: {avg_reward_dqn:.4f} // Winrate DQN: {winrate_dqn:.4f}\\nAvg Reward PPO: {avg_reward_ppo:.4f} // Winrate PPO: {winrate_ppo:.4f}\\nWinrate Draw: {winrate_draw:.4f}\")\n",
    "    return avg_reward_dqn, winrate_dqn, avg_reward_ppo, winrate_ppo, winrate_draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PPO CNN with 5, height 8, width 8\n",
      "Loading PPO CNN with 5, height 8, width 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/users/eleves-b/2022/gabriel.mercier/Cathedral-RL-CL/PPO/models/ppo_cnn.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action_mask_tensor = torch.tensor(action_mask, dtype=torch.bool)\n",
      "100%|██████████| 100/100 [00:42<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100 episodes:\n",
      "Avg Reward DQN: 0.0920 // Winrate DQN: 0.9500\n",
      "Avg Reward PPO: -1.4098 // Winrate PPO: 0.0400\n",
      "Winrate Draw: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "_, _, _, _, _= dqn_vs_ppo(model_dqn=\"True_False_model_DQN_episodes10000_buffer40000_prioritizedTrue_batch_size64_gamma0.95_target_update30_treshold_penalize0_boltzmann\", \n",
    "                          model_ppo=\"cathedral_ppo_self_play_adversarial_final\", \n",
    "                          board_size=8, \n",
    "                          device=device, \n",
    "                          num_episodes_eval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature = 1:\n",
    "Avg Reward DQN: -0.7753 // Winrate DQN: 0.7840\n",
    "Avg Reward PPO: -1.4818 // Winrate PPO: 0.1720\n",
    "Winrate Draw: 0.0440\n",
    "\n",
    "Temperature=5 for DQN\n",
    "100 episodes:\n",
    "Avg Reward DQN: -1.2714 // Winrate DQN: 0.5100\n",
    "Avg Reward PPO: -1.5022 // Winrate PPO: 0.3900\n",
    "Winrate Draw: 0.1000\n",
    "\n",
    "Temperature=0.3\n",
    "100 episodes:\n",
    "Avg Reward DQN: -0.0661 // Winrate DQN: 0.9600\n",
    "Avg Reward PPO: -1.4289 // Winrate PPO: 0.0400\n",
    "Winrate Draw: 0.0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_game_gif(episode_num, ppo_agent, dqn_model, gif_dir, board_size):\n",
    "        \"\"\"Create and save a GIF animation of a full game using Pygame rendering\"\"\"\n",
    "        \n",
    "        # Initialize rendering environment for this game\n",
    "        gif_env = cathedral_v0.env(board_size=board_size, render_mode=\"human\", \n",
    "                                  per_move_rewards=True, final_reward_score_difference=False)\n",
    "        gif_env.reset()\n",
    "        \n",
    "        # Initialize pygame if not already done\n",
    "        if pygame.get_init() == False:\n",
    "            pygame.init()\n",
    "        \n",
    "        # Store frames for the GIF\n",
    "        frames = []\n",
    "        \n",
    "        # Create a clock for controlling rendering speed\n",
    "        clock = pygame.time.Clock()\n",
    "        \n",
    "        # Capture initial board state\n",
    "        gif_env.render()\n",
    "        pygame.display.flip()\n",
    "        frame = pygame.surfarray.array3d(pygame.display.get_surface())\n",
    "        frame = np.transpose(frame, (1, 0, 2))  # Transpose to correct orientation\n",
    "        frames.append(Image.fromarray(frame))\n",
    "        player_0='player_0' # player_0 or player_1 (does not matter)\n",
    "        player_1='player_1'\n",
    "\n",
    "        # Game loop\n",
    "        count=0\n",
    "        while gif_env.agents:\n",
    "            current_agent = gif_env.agent_selection\n",
    "            observation = gif_env.observe(current_agent)\n",
    "            legal_moves = [i for i, valid in enumerate(observation[\"action_mask\"]) if valid]\n",
    "            state = observation[\"observation\"]\n",
    "            action_mask = observation[\"action_mask\"]\n",
    "            \n",
    "            # Get action using PPO\n",
    "            if current_agent == player_0:\n",
    "                 _, action = select_action_dqn(dqn_model, state, action_mask, legal_moves, device, 'boltzmann', 0, 0.1)\n",
    "            else:\n",
    "                action = ppo_agent.select_action_evaluation(state, action_mask)\n",
    "            \n",
    "            # Take step\n",
    "            gif_env.step(action)\n",
    "            \n",
    "            # Render and capture frame\n",
    "            gif_env.render()\n",
    "            pygame.display.flip()\n",
    "            \n",
    "            # Capture the displayed frame\n",
    "            frame = pygame.surfarray.array3d(pygame.display.get_surface())\n",
    "            frame = np.transpose(frame, (1, 0, 2))  # Transpose to correct orientation\n",
    "            frames.append(Image.fromarray(frame))\n",
    "            \n",
    "            # Control rendering speed\n",
    "            clock.tick(1)  # Limit to 2 FPS for the gif\n",
    "        \n",
    "        # Save frames as a GIF\n",
    "        gif_path = os.path.join(gif_dir, f\"game_episode_{episode_num}.gif\")\n",
    "        frames[0].save(\n",
    "            gif_path,\n",
    "            format='GIF',\n",
    "            append_images=frames[1:],\n",
    "            save_all=True,\n",
    "            duration=500,  # Duration between frames in milliseconds\n",
    "            loop=0  # Loop indefinitely\n",
    "        )\n",
    "        \n",
    "        # Close environment\n",
    "        gif_env.close()\n",
    "        print(f\"Game animation saved to {gif_path}\")\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PPO CNN with 5, height 8, width 8\n",
      "Loading PPO CNN with 5, height 8, width 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2022/gabriel.mercier/Cathedral-RL-CL/PPO/models/ppo_cnn.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action_mask_tensor = torch.tensor(action_mask, dtype=torch.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game animation saved to PPO_vs_DQN_gif/game_episode_0.gif\n",
      "Game animation saved to PPO_vs_DQN_gif/game_episode_1.gif\n",
      "Game animation saved to PPO_vs_DQN_gif/game_episode_2.gif\n",
      "Game animation saved to PPO_vs_DQN_gif/game_episode_3.gif\n",
      "Game animation saved to PPO_vs_DQN_gif/game_episode_4.gif\n",
      "Game animation saved to PPO_vs_DQN_gif/game_episode_5.gif\n",
      "Game animation saved to PPO_vs_DQN_gif/game_episode_6.gif\n",
      "Game animation saved to PPO_vs_DQN_gif/game_episode_7.gif\n",
      "Game animation saved to PPO_vs_DQN_gif/game_episode_8.gif\n",
      "Game animation saved to PPO_vs_DQN_gif/game_episode_9.gif\n"
     ]
    }
   ],
   "source": [
    "env = cathedral_v0.env(board_size=8, render_mode=\"text\", per_move_rewards=True, final_reward_score_difference=False)\n",
    "env.reset()\n",
    "\n",
    "player_0='player_0' # player_0 or player_1 (does not matter)\n",
    "player_1='player_1'\n",
    "n_actions = env.action_space(player_1).n\n",
    "obs_shape = env.observe(player_1)[\"observation\"].shape \n",
    "    \n",
    "ppo_agent = PPOCNN(\n",
    "        obs_shape=obs_shape,\n",
    "        action_dim=n_actions,\n",
    "        lr_actor=lr_actor, \n",
    "        lr_critic=lr_critic,\n",
    "        gamma=gamma,\n",
    "        K_epochs=K_epochs,\n",
    "        eps_clip=eps_clip\n",
    "    )\n",
    "checkpoint_ppo = torch.load(f\"models/cathedral_ppo_self_play_adversarial_final.pth\", weights_only=False)\n",
    "    \n",
    "if 'model_state_dict' in checkpoint_ppo:\n",
    "        ppo_agent.policy.load_state_dict(checkpoint_ppo['model_state_dict'])\n",
    "else:\n",
    "    # Direct state dict (from .save() method)\n",
    "    ppo_agent.policy.load_state_dict(checkpoint_ppo)\n",
    "    \n",
    "dqn_net = DQN(obs_shape, n_actions).to(device)\n",
    "\n",
    "checkpoint_dqn = torch.load(f\"models/True_False_model_DQN_episodes10000_buffer40000_prioritizedTrue_batch_size64_gamma0.95_target_update30_treshold_penalize0_boltzmann.pth\", weights_only=False)\n",
    "dqn_net.load_state_dict(checkpoint_dqn['model_state_dict'])\n",
    "   \n",
    "for episode in range(3):\n",
    "    create_game_gif(episode_num=episode, \n",
    "                    ppo_agent=ppo_agent, \n",
    "                    dqn_model=dqn_net, \n",
    "                    gif_dir=\"PPO_vs_DQN_gif\", \n",
    "                    board_size=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLenv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
