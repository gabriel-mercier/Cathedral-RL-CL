{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from cathedral_rl import cathedral_v0\n",
    "from cathedral_rl.game.manual_policy import ManualPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commandes \n",
    "\n",
    "\n",
    "Liste des touches possibles et leur effet :\n",
    "\n",
    "- Espace (K_SPACE) : Parcourt la liste des pièces disponibles en passant de la plus grande à la plus petite.\n",
    "- E (K_e) : Fait tourner la pièce dans le sens horaire (rotation à -90° par incrément, en tenant compte du plateau inversé).\n",
    "- Q (K_q) : Fait tourner la pièce dans le sens anti-horaire (rotation à +90° par incrément).\n",
    "- Flèche droite (K_RIGHT) : Déplace la pièce vers la droite, en vérifiant que le déplacement est légal.\n",
    "- Flèche gauche (K_LEFT) : Déplace la pièce vers la gauche, en vérifiant que le déplacement est légal.\n",
    "- Flèche haut (K_UP) : Déplace la pièce vers le haut (attention : en pygame, la coordonnée y augmente vers le bas), en vérifiant que le déplacement est légal.\n",
    "- Flèche bas (K_DOWN) : Déplace la pièce vers le bas, en vérifiant que le déplacement est légal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chose starting player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_player = \"human\" # human or AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choise AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_model = \"PPO\" # DQN or PPO or random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if starting_player == \"AI\":\n",
    "    controlled_agent = \"player_0\"\n",
    "    ai_agent = \"player_1\"\n",
    "else:\n",
    "    controlled_agent = \"player_1\"\n",
    "    ai_agent = \"player_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = cathedral_v0.env(\n",
    "    board_size=8,\n",
    "    render_mode=\"human\",\n",
    "    per_move_rewards=True,\n",
    "    final_reward_score_difference=True,\n",
    ").unwrapped\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PPO policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PPO.models.ppo_cnn import PPOCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ppo = \"cathedral_ppo_self_play_adversarial_final.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_epochs = 40               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.95                # discount factor\n",
    "gae_lambda = 0.95\n",
    "\n",
    "lr_actor = 0.005       # learning rate for actor network\n",
    "lr_critic = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/cathedral_ppo_self_play_adversarial_final.pth.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m n_actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space(ai_agent)\u001b[38;5;241m.\u001b[39mn\n\u001b[0;32m      2\u001b[0m obs_shape \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobserve(ai_agent)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \n\u001b[1;32m----> 4\u001b[0m checkpoint_ppo \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_ppo\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m ppo_agent \u001b[38;5;241m=\u001b[39m PPOCNN(\n\u001b[0;32m      6\u001b[0m     obs_shape\u001b[38;5;241m=\u001b[39mobs_shape,\n\u001b[0;32m      7\u001b[0m     action_dim\u001b[38;5;241m=\u001b[39mn_actions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     eps_clip\u001b[38;5;241m=\u001b[39meps_clip\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m checkpoint_ppo:\n",
      "File \u001b[1;32mc:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\simon\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/cathedral_ppo_self_play_adversarial_final.pth.pth'"
     ]
    }
   ],
   "source": [
    "n_actions = env.action_space(ai_agent).n\n",
    "obs_shape = env.observe(ai_agent)[\"observation\"].shape \n",
    "\n",
    "checkpoint_ppo = torch.load(f\"models/{model_ppo}.pth\", weights_only=False)\n",
    "ppo_agent = PPOCNN(\n",
    "    obs_shape=obs_shape,\n",
    "    action_dim=n_actions,\n",
    "    lr_actor=lr_actor, \n",
    "    lr_critic=lr_critic,\n",
    "    gamma=gamma,\n",
    "    K_epochs=K_epochs,\n",
    "    eps_clip=eps_clip\n",
    ")\n",
    "if 'model_state_dict' in checkpoint_ppo:\n",
    "    ppo_agent.policy.load_state_dict(checkpoint_ppo['model_state_dict'])\n",
    "else:\n",
    "    # Direct state dict (from .save() method)\n",
    "    ppo_agent.policy.load_state_dict(checkpoint_ppo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get correct action method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_action(observation, legal_moves):\n",
    "    if ai_model == \"random\":\n",
    "        return np.random.choice(legal_moves)\n",
    "    if ai_model == \"PPO\":\n",
    "        return ppo_agent.select_action_evaluation(observation[\"observation\"], observation[\"action_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play against AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Turn: 1 | (player_1) Legal pieces : [14], Legal moves total: 120, Remaining pieces: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 17:25:58.682 python[42405:6182076] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-12 17:25:58.682 python[42405:6182076] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn: 1 | Action: 1678, Piece: 14, Position: (3, 3), \n",
      "Turn: 1 | Reward: 0, Cumulative reward: 0, \n",
      "\n",
      "SCORE (player_1): 0.00, Squares/turn: 0.00, Remaining pieces difference: 0, Territory difference: 0\n",
      "SCORE (player_0): 0.00, Squares/turn: 0.00, Remaining pieces difference: 0, Territory difference: 0\n",
      "\n",
      "Turn: 2 | (player_0) Legal pieces : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], Legal moves total: 1139, Remaining pieces: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "Turn: 2 | Action: 1489, Piece: 13, Position: (1, 1), \n",
      "Turn: 2 | Reward: 3, Cumulative reward: 3, \n",
      "\n",
      "Turn: 3 | (player_1) Legal pieces : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], Legal moves total: 924, Remaining pieces: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "Turn: 3 | Action: 1612, Piece: 13, Position: (6, 1), \n",
      "Turn: 3 | Reward: 3, Cumulative reward: 3, \n",
      "\n",
      "SCORE (player_1): 5.00, Squares/turn: 5.00, Remaining pieces difference: 0, Territory difference: 0\n",
      "SCORE (player_0): 5.00, Squares/turn: 5.00, Remaining pieces difference: 0, Territory difference: 0\n",
      "\n",
      "Turn: 4 | (player_0) Legal pieces : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], Legal moves total: 641, Remaining pieces: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Turn: 4 | Action: 1400, Piece: 12, Position: (3, 6), \n",
      "Turn: 4 | Reward: 1, Cumulative reward: 7, \n",
      "\n",
      "Turn: 5 | (player_1) Legal pieces : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], Legal moves total: 467, Remaining pieces: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Turn: 5 | Action: 1347, Piece: 12, Position: (1, 5), \n",
      "Turn: 5 | Reward: 1, Cumulative reward: 7, \n",
      "\n",
      "SCORE (player_1): 5.00, Squares/turn: 5.00, Remaining pieces difference: 0, Territory difference: 0\n",
      "SCORE (player_0): 5.00, Squares/turn: 5.00, Remaining pieces difference: 0, Territory difference: 0\n",
      "\n",
      "Turn: 6 | (player_0) Legal pieces : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], Legal moves total: 336, Remaining pieces: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Turn: 6 | Action: 1282, Piece: 10, Position: (6, 6), \n",
      "Turn: 6 | Reward: 3, Cumulative reward: 11, \n",
      "\n",
      "Turn: 7 | (player_1) Legal pieces : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], Legal moves total: 179, Remaining pieces: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Turn: 7 | Action: 521, Piece: 4, Position: (6, 4), \n",
      "Turn: 7 | Reward: -1, Cumulative reward: 7, \n",
      "\n",
      "SCORE (player_1): 0.33, Squares/turn: 4.33, Remaining pieces difference: -2, Territory difference: -2\n",
      "SCORE (player_0): 9.00, Squares/turn: 5.00, Remaining pieces difference: 2, Territory difference: 2\n",
      "\n",
      "Turn: 8 | (player_0) Legal pieces : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], Legal moves total: 127, Remaining pieces: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n",
      "Turn: 8 | Action: 1103, Piece: 9, Position: (4, 1), \n",
      "Turn: 8 | Reward: 0, Cumulative reward: 14, \n",
      "\n",
      "Turn: 9 | (player_1) Legal pieces : [0, 1, 2, 3, 5, 6], Legal moves total: 66, Remaining pieces: [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11]\n",
      "Turn: 9 | Action: 79, Piece: 1, Position: (1, 7), \n",
      "Turn: 9 | Reward: -1, Cumulative reward: 5, \n",
      "\n",
      "SCORE (player_1): -2.50, Squares/turn: 3.50, Remaining pieces difference: -5, Territory difference: -1\n",
      "SCORE (player_0): 10.75, Squares/turn: 4.75, Remaining pieces difference: 5, Territory difference: 1\n",
      "\n",
      "Turn: 10 | (player_0) Legal pieces : [0, 1, 2, 3, 4, 5, 6], Legal moves total: 70, Remaining pieces: [0, 1, 2, 3, 4, 5, 6, 7, 8, 11]\n",
      "Turn: 10 | Action: 498, Piece: 4, Position: (5, 5), \n",
      "Turn: 10 | Reward: 0, Cumulative reward: 14, \n",
      "\n",
      "Turn: 11 | (player_1) Legal pieces : [0, 2, 3, 5, 6], Legal moves total: 32, Remaining pieces: [0, 2, 3, 5, 6, 7, 8, 9, 10, 11]\n",
      "Turn: 11 | Action: 286, Piece: 3, Position: (3, 0), \n",
      "Turn: 11 | Reward: -1, Cumulative reward: 3, \n",
      "\n",
      "SCORE (player_1): -3.80, Squares/turn: 3.20, Remaining pieces difference: -6, Territory difference: -1\n",
      "SCORE (player_0): 11.40, Squares/turn: 4.40, Remaining pieces difference: 6, Territory difference: 1\n",
      "\n",
      "Turn: 12 | (player_0) Legal pieces : [0, 1, 2, 3, 5, 6], Legal moves total: 45, Remaining pieces: [0, 1, 2, 3, 5, 6, 7, 8, 11]\n",
      "Turn: 12 | Action: 262, Piece: 3, Position: (1, 3), \n",
      "Turn: 12 | Reward: -1, Cumulative reward: 13, \n",
      "\n",
      "Turn: 13 | (player_1) Legal pieces : [0, 2, 5], Legal moves total: 15, Remaining pieces: [0, 2, 5, 6, 7, 8, 9, 10, 11]\n",
      "Turn: 13 | Action: 195, Piece: 2, Position: (4, 3), \n",
      "Turn: 13 | Reward: -1, Cumulative reward: 1, \n",
      "\n",
      "SCORE (player_1): -4.00, Squares/turn: 3.00, Remaining pieces difference: -6, Territory difference: -1\n",
      "SCORE (player_0): 11.00, Squares/turn: 4.00, Remaining pieces difference: 6, Territory difference: 1\n",
      "\n",
      "Turn: 14 | (player_0) Legal pieces : [0, 1, 2, 5], Legal moves total: 24, Remaining pieces: [0, 1, 2, 5, 6, 7, 8, 11]\n",
      "Turn: 14 | Action: 85, Piece: 1, Position: (2, 5), \n",
      "Turn: 14 | Reward: -2, Cumulative reward: 10, \n",
      "\n",
      "Turn: 15 | (player_1) Legal pieces : [0, 5], Legal moves total: 9, Remaining pieces: [0, 5, 6, 7, 8, 9, 10, 11]\n",
      "Turn: 15 | Action: 48, Piece: 0, Position: (6, 0), \n",
      "Turn: 15 | Reward: -2, Cumulative reward: -2, \n",
      "\n",
      "SCORE (player_1): -4.29, Squares/turn: 2.71, Remaining pieces difference: -6, Territory difference: -1\n",
      "SCORE (player_0): 10.57, Squares/turn: 3.57, Remaining pieces difference: 6, Territory difference: 1\n",
      "\n",
      "Turn: 16 | (player_0) Legal pieces : [0, 2, 5], Legal moves total: 13, Remaining pieces: [0, 2, 5, 6, 7, 8, 11]\n",
      "Turn: 16 | Action: 1, Piece: 0, Position: (0, 1), \n",
      "Turn: 16 | Reward: -2, Cumulative reward: 6, \n",
      "\n",
      "Turn: 17 | (player_0) Legal pieces : [2], Legal moves total: 2, Remaining pieces: [2, 5, 6, 7, 8, 11]\n",
      "Turn: 17 | Action: 129, Piece: 2, Position: (0, 0), \n",
      "Turn: 17 | Reward: 0, Cumulative reward: 6, \n",
      "Truncated\n",
      "\n",
      "WINNER:  1\n",
      "\n",
      "player_0 Final reward: 0\n",
      "player_0 Cumulative reward: 6\n",
      "player_0 Final remaining pieces: ['Inn2', 'Bridge', 'Square', 'Manor', 'Infirmary']\n",
      "player_0 Score: 13.11, Squares/turn: 3.11, Remaining pieces difference: 9, Territory difference: 1\n",
      "\n",
      "player_1 Final reward: -2\n",
      "player_1 Cumulative reward: -6\n",
      "player_1 Final remaining pieces: ['Inn2', 'Bridge', 'Square', 'Manor', 'Abbey', 'Academy', 'Infirmary']\n",
      "player_1 Score: -7.29, Squares/turn: 2.71, Remaining pieces difference: -9, Territory difference: -1\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "\n",
    "iter = 1\n",
    "\n",
    "# Agent_id can be 0 or 1 : indicates starting player\n",
    "if starting_player == \"AI\":\n",
    "    human_agent_id = 1\n",
    "else:\n",
    "    human_agent_id = 0\n",
    "\n",
    "manual_policy = ManualPolicy(env, agent_id=human_agent_id) # Policy controlled by player\n",
    "\n",
    "while env.agents:\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    mask = observation[\"action_mask\"]\n",
    "    legal_moves = [i for i, valid in enumerate(observation[\"action_mask\"]) if valid]\n",
    "    agent = env.agent_selection\n",
    "    state = observation[\"observation\"]\n",
    "\n",
    "    print(\n",
    "        f\"\\nTurn: {iter} | ({agent}) \"\n",
    "        f\"Legal pieces : {list(env.legal_pieces[agent])}, \"\n",
    "        f\"Legal moves total: {np.count_nonzero(mask)}, \"\n",
    "        f\"Remaining pieces: {env.board.unplaced_pieces[agent]}\"\n",
    "    )\n",
    "\n",
    "    if agent == manual_policy.agent:                # Human action\n",
    "        action = manual_policy(observation, agent)\n",
    "    else:                                           # AI action\n",
    "        state = observation[\"observation\"]\n",
    "        action = ai_action(observation, legal_moves)\n",
    "\n",
    "    env.step(action)\n",
    "\n",
    "    print(\n",
    "        f\"Turn: {iter} | \"\n",
    "        f\"Action: {action}, \"\n",
    "        f\"Piece: {env.board.action_to_piece_map(action)[0]}, \"\n",
    "        f\"Position: {env.board.action_to_pos_rotation_mapp(agent, action)[0]}, \"\n",
    "    )\n",
    "    print(\n",
    "        f\"Turn: {iter} | Reward: {env.rewards[agent]}, \"\n",
    "        f\"Cumulative reward: {env._cumulative_rewards[agent]}, \"\n",
    "    )\n",
    "    if env.turns[\"player_0\"] == env.turns[\"player_1\"]:\n",
    "        print()\n",
    "        for agent in env.agents:\n",
    "            print(\n",
    "                f\"SCORE ({agent}): {env.score[agent]['total']:0.2f}, \"\n",
    "                f\"Squares/turn: {env.score[agent]['squares_per_turn']:0.2f}, \"\n",
    "                f\"Remaining pieces difference: {env.score[agent]['remaining_pieces']}, \"\n",
    "                f\"Territory difference: {env.score[agent]['territory']}\"\n",
    "            )\n",
    "\n",
    "    iter += 1\n",
    "\n",
    "print(\"Terminated\") if termination else print(\"Truncated\")\n",
    "print(\"\\nWINNER: \", env.winner)\n",
    "for agent in env.possible_agents:\n",
    "    print(f\"\\n{agent} Final reward: {env.rewards[agent]}\")\n",
    "    print(f\"{agent} Cumulative reward: {env._cumulative_rewards[agent]}\")\n",
    "    print(\n",
    "        f\"{agent} Final remaining pieces: {[p.name for p in env.final_pieces[agent]]}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{agent} Score: {env.score[agent]['total']:0.2f}, \"\n",
    "        f\"Squares/turn: {env.score[agent]['squares_per_turn']:0.2f}, \"\n",
    "        f\"Remaining pieces difference: {env.score[agent]['remaining_pieces']}, \"\n",
    "        f\"Territory difference: {env.score[agent]['territory']}\"\n",
    "    )\n",
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cathedral_rl_kernel",
   "language": "python",
   "name": "cathedral_rl_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
