{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pygame\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from cathedral_rl import cathedral_v0\n",
    "from cathedral_rl.game.manual_policy import ManualPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commandes \n",
    "\n",
    "\n",
    "Liste des touches possibles et leur effet :\n",
    "\n",
    "- Espace (K_SPACE) : Parcourt la liste des pièces disponibles en passant de la plus grande à la plus petite.\n",
    "- E (K_e) : Fait tourner la pièce dans le sens horaire (rotation à -90° par incrément, en tenant compte du plateau inversé).\n",
    "- Q (K_q) : Fait tourner la pièce dans le sens anti-horaire (rotation à +90° par incrément).\n",
    "- Flèche droite (K_RIGHT) : Déplace la pièce vers la droite, en vérifiant que le déplacement est légal.\n",
    "- Flèche gauche (K_LEFT) : Déplace la pièce vers la gauche, en vérifiant que le déplacement est légal.\n",
    "- Flèche haut (K_UP) : Déplace la pièce vers le haut (attention : en pygame, la coordonnée y augmente vers le bas), en vérifiant que le déplacement est légal.\n",
    "- Flèche bas (K_DOWN) : Déplace la pièce vers le bas, en vérifiant que le déplacement est légal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chose starting player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_player = \"human\" # human or AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if starting_player == \"AI\":\n",
    "    controlled_agent = \"player_0\"\n",
    "else:\n",
    "    controlled_agent = \"player_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DQN policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_size = 8\n",
    "\n",
    "factor_illegal_action = 1\n",
    "\n",
    "parameters_updates = 10    # plusieurs mises à jour par épisode\n",
    "target_update_freq = 20    # fréquence (en épisodes) de mise à jour du réseau cible\n",
    "opponent_update_freq = 100\n",
    "\n",
    "epsilon_start = 0.3\n",
    "epsilon_final = 0.1\n",
    "epsilon_decay = 100    \n",
    "epsilon_opponent = 0.1  # faible exploration pour l'adversaire \n",
    "\n",
    "method = \"eps_greedy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # observations : (10, 10, 5)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(obs_shape[2], 32, kernel_size=3, stride=1, padding=1),  # output: 32 x 10 x 10\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),              # output: 64 x 10 x 10\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        dummy = torch.zeros(1, obs_shape[2], obs_shape[0], obs_shape[1])\n",
    "        conv_out_size = self.conv(dummy).shape[1]\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x(batch, hauteur, largeur, channels)\n",
    "        x = x.permute(0, 3, 1, 2)  \n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_by_episode(episode):\n",
    "    return epsilon_final + (epsilon_start - epsilon_final) * np.exp(-episode / epsilon_decay)\n",
    "\n",
    "def temperature_by_episode(episode):\n",
    "    return 1\n",
    "\n",
    "def select_action_dqn(model, obs, action_mask, legal_moves, episode, device, method, verbose=False):\n",
    "    model.eval()\n",
    "    not_legal_action = 0\n",
    "    with torch.no_grad():\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)  # (1, 10, 10, 5)\n",
    "        q_values = model(obs_tensor).squeeze(0)  # (n_actions,)\n",
    "        \n",
    "        if method == 'eps_greedy':\n",
    "            epsilon = epsilon_by_episode(episode) # epsilon-greedy\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice(legal_moves)\n",
    "                not_legal_action = 0\n",
    "            else:\n",
    "                first_action = torch.argmax(q_values).item()\n",
    "                mask = torch.tensor(action_mask, dtype=torch.bool, device=device)\n",
    "                q_values[~mask] = -1e8\n",
    "                action = torch.argmax(q_values).item()\n",
    "                not_legal_action = int(first_action != action)\n",
    "        \n",
    "        elif method == 'boltzmann':\n",
    "            temperature = temperature_by_episode(episode)\n",
    "            first_action = torch.argmax(q_values).item()\n",
    "            mask = torch.tensor(action_mask, dtype=torch.bool, device=device)\n",
    "            q_values[~mask] = -1e8\n",
    "            action = torch.argmax(q_values).item()\n",
    "            not_legal_action = int(first_action != action)\n",
    "            probabilities = F.softmax(q_values / temperature, dim=-1)\n",
    "            action = torch.multinomial(probabilities, num_samples=1).item()\n",
    "        \n",
    "            \n",
    "            \n",
    "    model.train()\n",
    "    return action, not_legal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num checkpoints: 0\n"
     ]
    }
   ],
   "source": [
    "env = cathedral_v0.env(\n",
    "    board_size=10,\n",
    "    render_mode=\"human\",\n",
    "    per_move_rewards=True,\n",
    "    final_reward_score_difference=True,\n",
    ").unwrapped\n",
    "\n",
    "env.reset()\n",
    "\n",
    "n_actions = env.action_space(controlled_agent).n\n",
    "obs_shape = env.observe(controlled_agent)[\"observation\"].shape  # (10, 10, 5)\n",
    "list_reward_training, policy_net_checkpoints, num_checkpoints = [], [], 0\n",
    "\n",
    "checkpoint = torch.load(\"model_weights_DQN/test5.pth\", weights_only=False)\n",
    "\n",
    "policy_net = DQN(obs_shape, n_actions).to(device)\n",
    "policy_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "list_reward_training = checkpoint['list_reward']\n",
    "policy_net_checkpoints = checkpoint['policy_net_checkpoints']\n",
    "num_checkpoints = checkpoint['num_checkpoints']\n",
    "print(f'Num checkpoints: {num_checkpoints}')\n",
    "\n",
    "list_reward = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play against AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Turn: 1 | (player_0) Legal pieces : [14], Legal moves total: 224, Remaining pieces: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Turn: 1 | Action: 2772, Piece: 14, Position: (2, 2), \n",
      "Turn: 1 | Reward: 0, Cumulative reward: 0, \n",
      "\n",
      "SCORE (player_0): 0.00, Squares/turn: 0.00, Remaining pieces difference: 0, Territory difference: 0\n",
      "SCORE (player_1): 0.00, Squares/turn: 0.00, Remaining pieces difference: 0, Territory difference: 0\n",
      "\n",
      "Turn: 2 | (player_1) Legal pieces : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], Legal moves total: 2366, Remaining pieces: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "Turn: 2 | Action: 1573, Piece: 8, Position: (4, 4), \n",
      "Turn: 2 | Reward: -1, Cumulative reward: -1, \n",
      "\n",
      "Turn: 3 | (player_0) Legal pieces : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], Legal moves total: 2034, Remaining pieces: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "Turn: 3 | Action: 2505, Piece: 13, Position: (1, 5), \n",
      "Turn: 3 | Reward: 0, Cumulative reward: 0, \n",
      "\n",
      "SCORE (player_0): 6.00, Squares/turn: 5.00, Remaining pieces difference: 1, Territory difference: 0\n",
      "SCORE (player_1): 3.00, Squares/turn: 4.00, Remaining pieces difference: -1, Territory difference: 0\n",
      "\n",
      "Turn: 4 | (player_1) Legal pieces : [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13], Legal moves total: 1601, Remaining pieces: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 13]\n",
      "Turn: 4 | Action: 850, Piece: 4, Position: (8, 5), \n",
      "Turn: 4 | Reward: -2, Cumulative reward: -4, \n",
      "\n",
      "Turn: 5 | (player_0) Legal pieces : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], Legal moves total: 1419, Remaining pieces: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Turn: 5 | Action: 2275, Piece: 12, Position: (2, 8), \n",
      "Turn: 5 | Reward: 4, Cumulative reward: 4, \n",
      "\n",
      "SCORE (player_0): 12.00, Squares/turn: 5.00, Remaining pieces difference: 3, Territory difference: 4\n",
      "SCORE (player_1): -3.50, Squares/turn: 3.50, Remaining pieces difference: -3, Territory difference: -4\n",
      "\n",
      "Turn: 6 | (player_1) Legal pieces : [0, 1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 13], Legal moves total: 1020, Remaining pieces: [0, 1, 2, 3, 5, 6, 7, 9, 10, 11, 12, 13]\n",
      "Turn: 6 | Action: 551, Piece: 3, Position: (9, 0), \n",
      "Turn: 6 | Reward: -3, Cumulative reward: -9, \n",
      "\n",
      "Turn: 7 | (player_0) Legal pieces : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], Legal moves total: 1077, Remaining pieces: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Turn: 7 | Action: 2169, Piece: 11, Position: (5, 1), \n",
      "Turn: 7 | Reward: 0, Cumulative reward: 8, \n",
      "\n",
      "SCORE (player_0): 15.00, Squares/turn: 5.00, Remaining pieces difference: 6, Territory difference: 4\n",
      "SCORE (player_1): -7.00, Squares/turn: 3.00, Remaining pieces difference: -6, Territory difference: -4\n",
      "\n",
      "Turn: 8 | (player_1) Legal pieces : [0, 1, 2, 5, 6, 7, 9, 10, 11, 12, 13], Legal moves total: 707, Remaining pieces: [0, 1, 2, 5, 6, 7, 9, 10, 11, 12, 13]\n",
      "Turn: 8 | Action: 1302, Piece: 6, Position: (5, 7), \n",
      "Turn: 8 | Reward: -2, Cumulative reward: -14, \n",
      "\n",
      "Turn: 9 | (player_0) Legal pieces : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], Legal moves total: 696, Remaining pieces: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Turn: 9 | Action: 2077, Piece: 10, Position: (7, 2), \n",
      "Turn: 9 | Reward: 0, Cumulative reward: 8, \n",
      "\n",
      "SCORE (player_0): 17.00, Squares/turn: 5.00, Remaining pieces difference: 8, Territory difference: 4\n",
      "SCORE (player_1): -9.00, Squares/turn: 3.00, Remaining pieces difference: -8, Territory difference: -4\n",
      "\n",
      "Turn: 10 | (player_1) Legal pieces : [0, 1, 2, 5, 7, 9, 10, 11, 12, 13], Legal moves total: 364, Remaining pieces: [0, 1, 2, 5, 7, 9, 10, 11, 12, 13]\n",
      "Turn: 10 | Action: 294, Piece: 2, Position: (4, 9), \n",
      "Turn: 10 | Reward: -3, Cumulative reward: -19, \n",
      "\n",
      "Turn: 11 | (player_0) Legal pieces : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], Legal moves total: 491, Remaining pieces: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Turn: 11 | Action: 374, Piece: 2, Position: (9, 3), \n",
      "Turn: 11 | Reward: 7, Cumulative reward: 15, \n",
      "\n",
      "SCORE (player_0): 25.40, Squares/turn: 4.40, Remaining pieces difference: 10, Territory difference: 11\n",
      "SCORE (player_1): -18.60, Squares/turn: 2.40, Remaining pieces difference: -10, Territory difference: -11\n",
      "\n",
      "Turn: 12 | (player_1) Legal pieces : [0, 1, 3, 5, 7, 9, 10, 11, 12, 13], Legal moves total: 310, Remaining pieces: [0, 1, 5, 7, 9, 10, 11, 12, 13, 3]\n",
      "Turn: 12 | Action: 36, Piece: 0, Position: (3, 6), \n",
      "Turn: 12 | Reward: -4, Cumulative reward: -25, \n",
      "\n",
      "Turn: 13 | (player_0) Legal pieces : [0, 1, 3, 4, 5, 6, 7, 8, 9], Legal moves total: 415, Remaining pieces: [0, 1, 3, 4, 5, 6, 7, 8, 9]\n",
      "Turn: 13 | Action: 1336, Piece: 6, Position: (7, 6), \n",
      "Turn: 13 | Reward: -1, Cumulative reward: 21, \n",
      "\n",
      "SCORE (player_0): 27.17, Squares/turn: 4.17, Remaining pieces difference: 12, Territory difference: 11\n",
      "SCORE (player_1): -20.83, Squares/turn: 2.17, Remaining pieces difference: -12, Territory difference: -11\n",
      "\n",
      "Turn: 14 | (player_1) Legal pieces : [1, 3, 5, 7, 9, 10, 11, 12, 13], Legal moves total: 193, Remaining pieces: [1, 5, 7, 9, 10, 11, 12, 13, 3]\n",
      "Turn: 14 | Action: 103, Piece: 1, Position: (0, 3), \n",
      "Turn: 14 | Reward: -4, Cumulative reward: -33, \n",
      "\n",
      "Turn: 15 | (player_0) Legal pieces : [0, 1, 3, 4, 5, 7, 8, 9], Legal moves total: 300, Remaining pieces: [0, 1, 3, 4, 5, 7, 8, 9]\n",
      "Turn: 15 | Action: 1838, Piece: 9, Position: (6, 4), \n",
      "Turn: 15 | Reward: 0, Cumulative reward: 20, \n",
      "\n",
      "SCORE (player_0): 30.14, Squares/turn: 4.14, Remaining pieces difference: 15, Territory difference: 11\n",
      "SCORE (player_1): -24.00, Squares/turn: 2.00, Remaining pieces difference: -15, Territory difference: -11\n",
      "\n",
      "Turn: 16 | (player_1) Legal pieces : [3, 5, 7, 9, 10, 11, 12, 13], Legal moves total: 112, Remaining pieces: [5, 7, 9, 10, 11, 12, 13, 3]\n",
      "Turn: 16 | Action: 1207, Piece: 5, Position: (9, 9), \n",
      "Turn: 16 | Reward: -2, Cumulative reward: -39, \n",
      "\n",
      "Turn: 17 | (player_0) Legal pieces : [0, 1, 3, 4, 5, 7, 8], Legal moves total: 211, Remaining pieces: [0, 1, 3, 4, 5, 7, 8]\n",
      "Turn: 17 | Action: 557, Piece: 3, Position: (9, 6), \n",
      "Turn: 17 | Reward: 5, Cumulative reward: 25, \n",
      "\n",
      "SCORE (player_0): 35.88, Squares/turn: 3.88, Remaining pieces difference: 17, Territory difference: 15\n",
      "SCORE (player_1): -30.25, Squares/turn: 1.75, Remaining pieces difference: -17, Territory difference: -15\n",
      "\n",
      "Turn: 18 | (player_1) Legal pieces : [3, 4, 7, 9, 10, 11, 12, 13], Legal moves total: 62, Remaining pieces: [7, 9, 10, 11, 12, 13, 3, 4]\n",
      "Turn: 18 | Action: 454, Piece: 3, Position: (3, 8), \n",
      "Turn: 18 | Reward: -1, Cumulative reward: -43, \n",
      "\n",
      "Turn: 19 | (player_0) Legal pieces : [0, 1, 4, 5, 7, 8], Legal moves total: 162, Remaining pieces: [0, 1, 4, 5, 7, 8]\n",
      "Turn: 19 | Action: 1656, Piece: 8, Position: (6, 8), \n",
      "Turn: 19 | Reward: 0, Cumulative reward: 30, \n",
      "\n",
      "SCORE (player_0): 39.89, Squares/turn: 3.89, Remaining pieces difference: 19, Territory difference: 17\n",
      "SCORE (player_1): -34.22, Squares/turn: 1.78, Remaining pieces difference: -19, Territory difference: -17\n",
      "\n",
      "Turn: 20 | (player_1) Legal pieces : [4, 9, 12], Legal moves total: 11, Remaining pieces: [7, 9, 10, 11, 12, 13, 4]\n",
      "Turn: 20 | Action: 564, Piece: 4, Position: (0, 2), \n",
      "Turn: 20 | Reward: -2, Cumulative reward: -46, \n",
      "\n",
      "Turn: 21 | (player_0) Legal pieces : [0, 1, 4, 5, 7], Legal moves total: 111, Remaining pieces: [0, 1, 4, 5, 7]\n",
      "Turn: 21 | Action: 567, Piece: 4, Position: (0, 4), \n",
      "Turn: 21 | Reward: -1, Cumulative reward: 29, \n",
      "\n",
      "SCORE (player_0): 39.80, Squares/turn: 3.80, Remaining pieces difference: 19, Territory difference: 17\n",
      "SCORE (player_1): -34.10, Squares/turn: 1.90, Remaining pieces difference: -19, Territory difference: -17\n",
      "\n",
      "Turn: 22 | (player_0) Legal pieces : [0, 1, 5, 7], Legal moves total: 88, Remaining pieces: [0, 1, 5, 7]\n",
      "Turn: 22 | Action: 140, Piece: 1, Position: (4, 0), \n",
      "Turn: 22 | Reward: -3, Cumulative reward: 26, \n",
      "\n",
      "Turn: 23 | (player_0) Legal pieces : [0, 5, 7], Legal moves total: 50, Remaining pieces: [0, 5, 7]\n",
      "Turn: 23 | Action: 980, Piece: 5, Position: (3, 2), \n",
      "Turn: 23 | Reward: -1, Cumulative reward: 25, \n",
      "\n",
      "Turn: 24 | (player_0) Legal pieces : [0, 7], Legal moves total: 34, Remaining pieces: [0, 7]\n",
      "Turn: 24 | Action: 0, Piece: 0, Position: (0, 0), \n",
      "Turn: 24 | Reward: -3, Cumulative reward: 22, \n",
      "\n",
      "Turn: 25 | (player_0) Legal pieces : [7], Legal moves total: 1, Remaining pieces: [7]\n",
      "Turn: 25 | Action: 1440, Piece: 7, Position: (8, 0), \n",
      "Turn: 25 | Reward: 0, Cumulative reward: 22, \n",
      "Truncated\n",
      "\n",
      "WINNER:  0\n",
      "\n",
      "player_0 Final reward: 0\n",
      "player_0 Cumulative reward: 22\n",
      "player_0 Final remaining pieces: []\n",
      "player_0 Score: 48.36, Squares/turn: 3.36, Remaining pieces difference: 28, Territory difference: 17\n",
      "\n",
      "player_1 Final reward: -2\n",
      "player_1 Cumulative reward: -56\n",
      "player_1 Final remaining pieces: ['Square', 'AbbeyFlipped', 'AcademyFlipped', 'Infirmary', 'Castle', 'Tower']\n",
      "player_1 Score: -43.10, Squares/turn: 1.90, Remaining pieces difference: -28, Territory difference: -17\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "\n",
    "iter = 1\n",
    "\n",
    "# Agent_id can be 0 or 1 : indicates starting player\n",
    "if starting_player == \"AI\":\n",
    "    human_agent_id = 1\n",
    "else:\n",
    "    human_agent_id = 0\n",
    "\n",
    "manual_policy = ManualPolicy(env, agent_id=human_agent_id) # Policy controlled by player\n",
    "\n",
    "while env.agents:\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    mask = observation[\"action_mask\"]\n",
    "    legal_moves = [i for i, valid in enumerate(observation[\"action_mask\"]) if valid]\n",
    "    agent = env.agent_selection\n",
    "\n",
    "    print(\n",
    "        f\"\\nTurn: {iter} | ({agent}) \"\n",
    "        f\"Legal pieces : {list(env.legal_pieces[agent])}, \"\n",
    "        f\"Legal moves total: {np.count_nonzero(mask)}, \"\n",
    "        f\"Remaining pieces: {env.board.unplaced_pieces[agent]}\"\n",
    "    )\n",
    "\n",
    "    if agent == manual_policy.agent:                # Human action\n",
    "        action = manual_policy(observation, agent)\n",
    "    else:                                           # AI action\n",
    "        state = observation[\"observation\"]\n",
    "        action, _ = select_action_dqn(policy_net, state, mask, legal_moves, 0, device, method)\n",
    "\n",
    "    env.step(action)\n",
    "\n",
    "    print(\n",
    "        f\"Turn: {iter} | \"\n",
    "        f\"Action: {action}, \"\n",
    "        f\"Piece: {env.board.action_to_piece_map(action)[0]}, \"\n",
    "        f\"Position: {env.board.action_to_pos_rotation_mapp(agent, action)[0]}, \"\n",
    "    )\n",
    "    print(\n",
    "        f\"Turn: {iter} | Reward: {env.rewards[agent]}, \"\n",
    "        f\"Cumulative reward: {env._cumulative_rewards[agent]}, \"\n",
    "    )\n",
    "    if env.turns[\"player_0\"] == env.turns[\"player_1\"]:\n",
    "        print()\n",
    "        for agent in env.agents:\n",
    "            print(\n",
    "                f\"SCORE ({agent}): {env.score[agent]['total']:0.2f}, \"\n",
    "                f\"Squares/turn: {env.score[agent]['squares_per_turn']:0.2f}, \"\n",
    "                f\"Remaining pieces difference: {env.score[agent]['remaining_pieces']}, \"\n",
    "                f\"Territory difference: {env.score[agent]['territory']}\"\n",
    "            )\n",
    "\n",
    "    iter += 1\n",
    "\n",
    "print(\"Terminated\") if termination else print(\"Truncated\")\n",
    "print(\"\\nWINNER: \", env.winner)\n",
    "for agent in env.possible_agents:\n",
    "    print(f\"\\n{agent} Final reward: {env.rewards[agent]}\")\n",
    "    print(f\"{agent} Cumulative reward: {env._cumulative_rewards[agent]}\")\n",
    "    print(\n",
    "        f\"{agent} Final remaining pieces: {[p.name for p in env.final_pieces[agent]]}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{agent} Score: {env.score[agent]['total']:0.2f}, \"\n",
    "        f\"Squares/turn: {env.score[agent]['squares_per_turn']:0.2f}, \"\n",
    "        f\"Remaining pieces difference: {env.score[agent]['remaining_pieces']}, \"\n",
    "        f\"Territory difference: {env.score[agent]['territory']}\"\n",
    "    )\n",
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cathedral_rl_kernel",
   "language": "python",
   "name": "cathedral_rl_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
