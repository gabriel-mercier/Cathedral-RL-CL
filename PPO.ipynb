{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.distributions import Categorical\n",
                "import matplotlib.pyplot as plt\n",
                "from cathedral_rl import cathedral_v0  \n",
                "import random\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "################################## set device ##################################\n",
                "\n",
                "print(\"============================================================================================\")\n",
                "\n",
                "\n",
                "# set device to cpu or cuda\n",
                "device = torch.device('cpu')\n",
                "\n",
                "if(torch.cuda.is_available()): \n",
                "    device = torch.device('cuda:0') \n",
                "    torch.cuda.empty_cache()\n",
                "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
                "else:\n",
                "    print(\"Device set to : cpu\")\n",
                "    \n",
                "print(\"============================================================================================\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RolloutBuffer:\n",
                "    def __init__(self):\n",
                "        self.actions = []\n",
                "        self.states = []\n",
                "        self.logprobs = []\n",
                "        self.rewards = []\n",
                "        self.state_values = []\n",
                "        self.is_terminals = []\n",
                "        self.action_masks = []\n",
                "    \n",
                "    def clear(self):\n",
                "        del self.actions[:]\n",
                "        del self.states[:]\n",
                "        del self.logprobs[:]\n",
                "        del self.rewards[:]\n",
                "        del self.state_values[:]\n",
                "        del self.is_terminals[:]\n",
                "        del self.action_masks[:]\n",
                "\n",
                "\n",
                "class ActorCritic(nn.Module):\n",
                "    def __init__(self, obs_shape, action_dim):\n",
                "        super(ActorCritic, self).__init__()\n",
                "        \n",
                "        # Calculate input size from observation shape\n",
                "        self.state_dim = obs_shape[0] * obs_shape[1] * obs_shape[2]  # Flattened input\n",
                "        self.action_dim = action_dim\n",
                "\n",
                "        # Actor network - outputs action probabilities\n",
                "        self.actor = nn.Sequential(\n",
                "            nn.Linear(self.state_dim, 256),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(256, 256),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(256, action_dim)\n",
                "            # No softmax here - will apply it with action mask later\n",
                "        )\n",
                "        \n",
                "        # Critic network - outputs state value\n",
                "        self.critic = nn.Sequential(\n",
                "            nn.Linear(self.state_dim, 256),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(256, 256),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(256, 1)\n",
                "        )\n",
                "        \n",
                "    def forward(self):\n",
                "        raise NotImplementedError\n",
                "    \n",
                "    def act(self, state, action_mask):\n",
                "        # Flatten state for linear layers\n",
                "        state_flat = state.reshape(-1)\n",
                "        \n",
                "        # Get action logits from actor\n",
                "        action_logits = self.actor(state_flat)\n",
                "        \n",
                "        # Apply action mask (set invalid actions to negative infinity)\n",
                "        action_mask_tensor = torch.tensor(action_mask, dtype=torch.bool)\n",
                "        action_logits[~action_mask_tensor] = -1e8\n",
                "        \n",
                "        # Apply softmax to get action probabilities\n",
                "        action_probs = torch.softmax(action_logits, dim=0)\n",
                "        \n",
                "        # Create categorical distribution\n",
                "        dist = Categorical(action_probs)\n",
                "        \n",
                "        # Sample action\n",
                "        action = dist.sample()\n",
                "        action_logprob = dist.log_prob(action)\n",
                "        \n",
                "        # Get state value from critic\n",
                "        state_val = self.critic(state_flat)\n",
                "        \n",
                "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
                "    \n",
                "    def evaluate(self, states, actions, action_masks):\n",
                "        # Flatten states for linear layers\n",
                "        batch_size = states.shape[0]\n",
                "        states_flat = states.reshape(batch_size, -1)\n",
                "        \n",
                "        # Get action logits from actor for all states\n",
                "        action_logits = self.actor(states_flat)\n",
                "        \n",
                "        # Apply action masks (set invalid actions to negative infinity)\n",
                "        for i in range(batch_size):\n",
                "            action_mask = action_masks[i]\n",
                "            action_logits[i][~action_mask] = -1e8\n",
                "        \n",
                "        # Apply softmax to get action probabilities\n",
                "        action_probs = torch.softmax(action_logits, dim=1)\n",
                "        \n",
                "        # Create categorical distributions\n",
                "        dist = Categorical(action_probs)\n",
                "        \n",
                "        # Get log probabilities, entropies, and state values\n",
                "        action_logprobs = dist.log_prob(actions)\n",
                "        dist_entropy = dist.entropy()\n",
                "        state_values = self.critic(states_flat)\n",
                "        \n",
                "        return action_logprobs, state_values, dist_entropy\n",
                "\n",
                "\n",
                "class PPO:\n",
                "    def __init__(self, obs_shape, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip):\n",
                "        self.gamma = gamma\n",
                "        self.eps_clip = eps_clip\n",
                "        self.K_epochs = K_epochs\n",
                "        \n",
                "        self.buffer = RolloutBuffer()\n",
                "        \n",
                "        self.policy = ActorCritic(obs_shape, action_dim).to(device)\n",
                "        self.optimizer = torch.optim.Adam([\n",
                "            {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
                "            {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
                "        ])\n",
                "        \n",
                "        self.policy_old = ActorCritic(obs_shape, action_dim).to(device)\n",
                "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
                "        \n",
                "        self.MseLoss = nn.MSELoss()\n",
                "    \n",
                "    def select_action(self, state, action_mask):\n",
                "        with torch.no_grad():\n",
                "            state_tensor = torch.FloatTensor(state).to(device)\n",
                "            action_mask_tensor = torch.BoolTensor(action_mask).to(device)\n",
                "            action, action_logprob, state_val = self.policy_old.act(state_tensor, action_mask_tensor)\n",
                "        \n",
                "        self.buffer.states.append(state_tensor)\n",
                "        self.buffer.actions.append(action)\n",
                "        self.buffer.logprobs.append(action_logprob)\n",
                "        self.buffer.state_values.append(state_val)\n",
                "        self.buffer.action_masks.append(action_mask_tensor)\n",
                "        \n",
                "        return action.item()\n",
                "    \n",
                "    def update(self):\n",
                "        # Monte Carlo estimate of returns\n",
                "        rewards = []\n",
                "        discounted_reward = 0\n",
                "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
                "            #print(f'reward {reward}, is terminal {is_terminal}')\n",
                "            if is_terminal:\n",
                "                discounted_reward = 0\n",
                "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
                "            rewards.insert(0, discounted_reward)\n",
                "        \n",
                "        # Normalizing the rewards (if there are any)\n",
                "        if len(rewards) > 0:\n",
                "            rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
                "            #print(f' reward normalisation {rewards}')\n",
                "            if rewards.std() > 0:\n",
                "                rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
                "        else:\n",
                "            return  # Nothing to update if buffer is empty\n",
                "        \n",
                "        # Convert lists to tensors\n",
                "        old_states = torch.stack(self.buffer.states).detach().to(device)\n",
                "        old_actions = torch.stack(self.buffer.actions).detach().to(device)\n",
                "        old_logprobs = torch.stack(self.buffer.logprobs).detach().to(device)\n",
                "        old_state_values = torch.stack(self.buffer.state_values).detach().squeeze().to(device)\n",
                "        old_action_masks = torch.stack(self.buffer.action_masks).detach().to(device)\n",
                "        \n",
                "        # Calculate advantages\n",
                "        advantages = rewards - old_state_values\n",
                "        #print(f'Advantages {advantages.mean()}, rewards {rewards.mean()} old state values {old_state_values.mean()}')\n",
                "        \n",
                "        # Optimize policy for K epochs\n",
                "        for _ in range(self.K_epochs):\n",
                "            # Evaluating old actions and values\n",
                "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions, old_action_masks)\n",
                "            \n",
                "            # Match state_values tensor dimensions with rewards tensor\n",
                "            state_values = torch.squeeze(state_values)\n",
                "            \n",
                "            # Finding the ratio (pi_theta / pi_theta__old)\n",
                "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
                "            \n",
                "            # Finding Surrogate Loss\n",
                "            surr1 = ratios * advantages\n",
                "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
                "            mse_loss = self.MseLoss(state_values, rewards)\n",
                "            #print(f' surr1 {surr1.mean()}, surr2{surr2.mean()} mse loss * 0.5 {mse_loss.mean() * 0.5} dist entropy * 0.01 {0.01 * dist_entropy.mean()}')\n",
                "            \n",
                "            # Final loss of clipped objective PPO\n",
                "            loss = -torch.min(surr1, surr2) + 0.5 * mse_loss - 0.01 * dist_entropy\n",
                "            \n",
                "            # Take gradient step\n",
                "            self.optimizer.zero_grad()\n",
                "            loss.mean().backward()\n",
                "            self.optimizer.step()\n",
                "        \n",
                "        # Copy new weights into old policy\n",
                "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
                "        \n",
                "        # Clear buffer\n",
                "        self.buffer.clear()\n",
                "    \n",
                "    def save(self, checkpoint_path):\n",
                "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
                "    \n",
                "    def load(self, checkpoint_path):\n",
                "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
                "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "has_continuous_action_space = False\n",
                "\n",
                "K_epochs = 40               # update policy for K epochs\n",
                "eps_clip = 0.2              # clip parameter for PPO\n",
                "gamma = 0.95                # discount factor\n",
                "\n",
                "lr_actor = 0.005       # learning rate for actor network\n",
                "lr_critic = 0.002"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "board_size = 8\n",
                "num_episodes = 5000\n",
                "save_freq = 100\n",
                "factor_illegal_action = 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Pure self play of PPO vs itself"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_ppo_self_play(name):\n",
                "    env = cathedral_v0.env(board_size=board_size, render_mode=\"text\", per_move_rewards=True, final_reward_score_difference=False)\n",
                "    env.reset()\n",
                "    \n",
                "    # Get action space and observation shape (should be the same for both players)\n",
                "    player_0 = \"player_0\"\n",
                "    player_1 = \"player_1\"\n",
                "    n_actions = env.action_space(player_0).n\n",
                "    obs_shape = env.observe(player_0)[\"observation\"].shape  # (10, 10, 5)\n",
                "    \n",
                "    print(f'n_actions: {n_actions}')\n",
                "    print(f'observation shape: {obs_shape}')\n",
                "    \n",
                "    # Initialize PPO agent (will be used for both players)\n",
                "    ppo_agent = PPO(\n",
                "        obs_shape=obs_shape,\n",
                "        action_dim=n_actions,\n",
                "        lr_actor=lr_actor,\n",
                "        lr_critic=lr_critic,\n",
                "        gamma=gamma,\n",
                "        K_epochs=K_epochs,\n",
                "        eps_clip=eps_clip\n",
                "    )\n",
                "    \n",
                "    # Training statistics\n",
                "    list_reward_p0 = []\n",
                "    list_reward_p1 = []\n",
                "    policy_checkpoints = []\n",
                "    list_win_count_p0 = []\n",
                "    list_win_count_p1 = []\n",
                "    list_draw_count = []\n",
                "    \n",
                "    # Training loop\n",
                "    for episode in range(num_episodes):\n",
                "        print(f'Episode {episode}')\n",
                "        env.reset()\n",
                "        print(f\"Starting player: {env.agent_selection}\")\n",
                "        total_reward_p0 = 0\n",
                "        total_reward_p1 = 0\n",
                "        episode_timesteps = 0\n",
                "        \n",
                "        # Clear player buffer\n",
                "        ppo_agent.buffer.clear()\n",
                "        \n",
                "        while env.agents:\n",
                "            current_agent = env.agent_selection\n",
                "            observation = env.observe(current_agent)\n",
                "            state = observation[\"observation\"]\n",
                "            action_mask = observation[\"action_mask\"]\n",
                "            \n",
                "            # Both players use PPO to select actions\n",
                "            action = ppo_agent.select_action(state, action_mask)\n",
                "            \n",
                "            # Take action in environment\n",
                "            env.step(action)\n",
                "            \n",
                "            # Get reward\n",
                "            reward = env.rewards[current_agent]\n",
                "            \n",
                "            # Track total rewards\n",
                "            if current_agent == player_0:\n",
                "                total_reward_p0 += reward\n",
                "            else:\n",
                "                total_reward_p1 += reward\n",
                "            \n",
                "            # Store reward in appropriate buffer\n",
                "            ppo_agent.buffer.rewards.append(reward)\n",
                "            \n",
                "            # Check if episode is done\n",
                "            if current_agent not in env.agents:\n",
                "                ppo_agent.buffer.is_terminals.append(1)\n",
                "            else:\n",
                "                ppo_agent.buffer.is_terminals.append(0)\n",
                "            \n",
                "            episode_timesteps += 1\n",
                "        \n",
                "        print(f'Timesteps of the episode {episode_timesteps}')\n",
                "        \n",
                "        # Update PPO policy using both players' experiences\n",
                "        if episode >= 10:  # Start updating after collecting some experience\n",
                "            # Combine experiences from both players\n",
                "            ppo_agent.update()\n",
                "        \n",
                "        # Collect statistics\n",
                "        print(f'Total rewards p0 {total_reward_p0} and p1 {total_reward_p1}')\n",
                "        list_reward_p0.append(total_reward_p0)\n",
                "        list_reward_p1.append(total_reward_p1)\n",
                "        \n",
                "        # Track wins for player_0 (for consistency with the old tracking)\n",
                "        if not list_win_count_p0:\n",
                "            list_win_count_p0 = [0]\n",
                "        if not list_win_count_p1:  # You need this list too\n",
                "            list_win_count_p1 = [0]\n",
                "        if not list_draw_count:\n",
                "            list_draw_count = [0]\n",
                "\n",
                "        # Update the appropriate counter based on the game outcome\n",
                "        if env.winner == 0:  # player_0 wins\n",
                "            list_win_count_p0.append(list_win_count_p0[-1] + 1)\n",
                "            list_win_count_p1.append(list_win_count_p1[-1])  # Carry forward p1 count unchanged\n",
                "            list_draw_count.append(list_draw_count[-1])  # Carry forward draw count unchanged\n",
                "            \n",
                "        elif env.winner == 1:  # player_1 wins\n",
                "            list_win_count_p0.append(list_win_count_p0[-1])  # Carry forward p0 count unchanged\n",
                "            list_win_count_p1.append(list_win_count_p1[-1] + 1)\n",
                "            list_draw_count.append(list_draw_count[-1])  # Carry forward draw count unchanged\n",
                "            \n",
                "        elif env.winner == -1:  # draw\n",
                "            list_win_count_p0.append(list_win_count_p0[-1])  # Carry forward p0 count unchanged\n",
                "            list_win_count_p1.append(list_win_count_p1[-1])  # Carry forward p1 count unchanged\n",
                "            list_draw_count.append(list_draw_count[-1] + 1)\n",
                "        \n",
                "        if (episode + 1) % 100 == 0:\n",
                "            win_rate_p0 = list_win_count_p0[-1] / (episode + 1)\n",
                "            win_rate_p1 = list_win_count_p1[-1] / (episode + 1)\n",
                "            draw_rate = list_draw_count[-1] / (episode + 1)\n",
                "            \n",
                "            print(f\"Episode {episode+1}/{num_episodes}\")\n",
                "            print(f\"  Player 0 - Avg Reward: {sum(list_reward_p0[-100:]) / 100:.2f} - Win Rate: {win_rate_p0}\")\n",
                "            print(f\"  Player 1 - Avg Reward: {sum(list_reward_p1[-100:]) / 100:.2f} - Win Rate: {win_rate_p1}\")\n",
                "            print(f\"  Draw Rate: {draw_rate:.2f}\")\n",
                "            \n",
                "        # Save checkpoint\n",
                "        if (episode+1) % save_freq == 0:\n",
                "            policy_checkpoints.append(ppo_agent.policy.state_dict())\n",
                "            print(\"Checkpoint saved\")\n",
                "            \n",
                "            # Save model\n",
                "            os.makedirs(\"model_weights_PPO\", exist_ok=True)\n",
                "            ppo_agent.save(f\"model_weights_PPO/{name}_{episode+1}.pth\")\n",
                "    \n",
                "    # Save final model and training statistics\n",
                "    os.makedirs(\"model_weights_PPO\", exist_ok=True)\n",
                "    print(f'Saving final model')\n",
                "    torch.save({\n",
                "        'model_state_dict': ppo_agent.policy.state_dict(),\n",
                "        'list_reward_p0': list_reward_p0,\n",
                "        'list_reward_p1': list_reward_p1,\n",
                "        'list_win_count_p0':list_win_count_p0,\n",
                "        'list_draw_count': list_draw_count,\n",
                "        'policy_checkpoints': policy_checkpoints,\n",
                "        'num_checkpoints': len(policy_checkpoints)\n",
                "    }, f\"model_weights_PPO/{name}_final.pth\")\n",
                "    \n",
                "    env.close()\n",
                "    \n",
                "    return list_reward_p0, list_reward_p1, list_win_count_p0, list_win_count_p1, list_draw_count"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rewards_p0, rewards_p1, win_counts_p0, win_counts_p1, draw_counts = train_ppo_self_play(\"cathedral_ppo_self_play\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Playing against a random opponent"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_ppo_random(name):\n",
                "    env = cathedral_v0.env(board_size=board_size, render_mode=\"text\", per_move_rewards=True, final_reward_score_difference=False)\n",
                "    env.reset()\n",
                "    \n",
                "    # Get action space and observation shape (should be the same for both players)\n",
                "    player_0 = \"player_0\"\n",
                "    \n",
                "    n_actions = env.action_space(player_0).n\n",
                "    obs_shape = env.observe(player_0)[\"observation\"].shape  # (10, 10, 5)\n",
                "    \n",
                "    print(f'n_actions: {n_actions}')\n",
                "    print(f'observation shape: {obs_shape}')\n",
                "    \n",
                "    # Initialize PPO agent (will be used for both players)\n",
                "    ppo_agent = PPO(\n",
                "        obs_shape=obs_shape,\n",
                "        action_dim=n_actions,\n",
                "        lr_actor=lr_actor,\n",
                "        lr_critic=lr_critic,\n",
                "        gamma=gamma,\n",
                "        K_epochs=K_epochs,\n",
                "        eps_clip=eps_clip\n",
                "    )\n",
                "    \n",
                "    # Training statistics\n",
                "    list_reward_p0 = []\n",
                "    list_reward_p1 = []\n",
                "    policy_checkpoints = []\n",
                "    list_win_count_p0 = [0]  # Initialize with 0\n",
                "    list_win_count_p1 = [0]  # Initialize with 0\n",
                "    list_draw_count = [0]    # Initialize with 0\n",
                "    \n",
                "    # Training loop\n",
                "    for episode in range(num_episodes):\n",
                "        print(f'Episode {episode}')\n",
                "        env.reset()\n",
                "        print(f\"Starting player: {env.agent_selection}\")\n",
                "        total_reward_p0 = 0\n",
                "        total_reward_p1 = 0\n",
                "        episode_timesteps = 0\n",
                "        \n",
                "        # Clear player buffer\n",
                "        ppo_agent.buffer.clear()\n",
                "        \n",
                "        # Run the game until completion\n",
                "        while env.agents:  # Continue while there are still active agents\n",
                "            current_agent = env.agent_selection\n",
                "            episode_timesteps += 1\n",
                "            observation = env.observe(current_agent)\n",
                "            legal_moves = [i for i, valid in enumerate(observation[\"action_mask\"]) if valid]\n",
                "            \n",
                "            if current_agent == player_0:  # PPO agent's turn\n",
                "                \n",
                "                state = observation[\"observation\"]\n",
                "                action_mask = observation[\"action_mask\"]\n",
                "                \n",
                "                # Use PPO to select action\n",
                "                action = ppo_agent.select_action(state, action_mask)\n",
                "                \n",
                "                # Take action in environment\n",
                "                env.step(action)\n",
                "                \n",
                "                # Get reward and add to buffer\n",
                "                reward = env.rewards[current_agent]\n",
                "                print(f'reward {reward}')\n",
                "                ppo_agent.buffer.rewards.append(reward)\n",
                "                \n",
                "                # Track total rewards\n",
                "                total_reward_p0 += reward\n",
                "                \n",
                "                # Check if episode is done for this agent\n",
                "                if current_agent not in env.agents:\n",
                "                    ppo_agent.buffer.is_terminals.append(1)\n",
                "                    print(f'Appended 1 to terminal')\n",
                "                else:\n",
                "                    ppo_agent.buffer.is_terminals.append(0)\n",
                "                    print(f'Appended 0 to terminal')\n",
                "                \n",
                "            else:  # Random agent's turn\n",
                "                observation = env.observe(current_agent)\n",
                "                action_mask = observation[\"action_mask\"]\n",
                "                \n",
                "                # Random valid action\n",
                "        \n",
                "                action = np.random.choice(legal_moves)\n",
                "                \n",
                "                # Take action in environment\n",
                "                env.step(action)\n",
                "                \n",
                "                # Track total rewards (don't add to PPO buffer)\n",
                "                reward = env.rewards[current_agent]\n",
                "                print(f'reward {reward}')\n",
                "                \n",
                "                total_reward_p1 += reward\n",
                "        \n",
                "        # At this point, the game has ended\n",
                "        print(f'Timesteps of the episode: {episode_timesteps}')\n",
                "        \n",
                "        if current_agent == player_0:\n",
                "            # We've already counted player_0's reward, but we need to account for player_1's final reward\n",
                "            total_reward_p1 += env.rewards[\"player_1\"]\n",
                "        else:\n",
                "            # We've already counted player_1's reward, but we need to account for player_0's final reward\n",
                "            total_reward_p0 += env.rewards[\"player_0\"]\n",
                "            # Also update the PPO buffer with the correct final reward\n",
                "            if ppo_agent.buffer.rewards and len(ppo_agent.buffer.rewards) > 0:\n",
                "                # Replace the last \"0\" reward with the actual final reward\n",
                "                ppo_agent.buffer.rewards[-1] = env.rewards[\"player_0\"]\n",
                "                ppo_agent.buffer.is_terminals[-1] = 1\n",
                "            \n",
                "        ppo_agent.update()\n",
                "        \n",
                "        # Collect statistics\n",
                "        print(f'Total rewards p0 {total_reward_p0} and p1 {total_reward_p1}')\n",
                "        list_reward_p0.append(total_reward_p0)\n",
                "        list_reward_p1.append(total_reward_p1)\n",
                "        \n",
                "        # Update the appropriate counter based on the game outcome\n",
                "        if env.winner == 0:  # player_0 wins\n",
                "            list_win_count_p0.append(list_win_count_p0[-1] + 1)\n",
                "            list_win_count_p1.append(list_win_count_p1[-1])  # Carry forward p1 count unchanged\n",
                "            list_draw_count.append(list_draw_count[-1])  # Carry forward draw count unchanged\n",
                "            \n",
                "        elif env.winner == 1:  # player_1 wins\n",
                "            list_win_count_p0.append(list_win_count_p0[-1])  # Carry forward p0 count unchanged\n",
                "            list_win_count_p1.append(list_win_count_p1[-1] + 1)\n",
                "            list_draw_count.append(list_draw_count[-1])  # Carry forward draw count unchanged\n",
                "            \n",
                "        elif env.winner == -1:  # draw\n",
                "            list_win_count_p0.append(list_win_count_p0[-1])  # Carry forward p0 count unchanged\n",
                "            list_win_count_p1.append(list_win_count_p1[-1])  # Carry forward p1 count unchanged\n",
                "            list_draw_count.append(list_draw_count[-1] + 1)\n",
                "        \n",
                "        if (episode + 1) % 100 == 0:\n",
                "            win_rate_p0 = list_win_count_p0[-1] / (episode + 1)\n",
                "            win_rate_p1 = list_win_count_p1[-1] / (episode + 1)\n",
                "            draw_rate = list_draw_count[-1] / (episode + 1)\n",
                "            \n",
                "            print(f\"Episode {episode+1}/{num_episodes}\")\n",
                "            print(f\"  Player 0 - Avg Reward: {sum(list_reward_p0[-100:]) / 100:.2f} - Win Rate: {win_rate_p0}\")\n",
                "            print(f\"  Player 1 - Avg Reward: {sum(list_reward_p1[-100:]) / 100:.2f} - Win Rate: {win_rate_p1}\")\n",
                "            print(f\"  Draw Rate: {draw_rate:.2f}\")\n",
                "            \n",
                "        # Save checkpoint\n",
                "        if (episode+1) % 500 == 0:\n",
                "            policy_checkpoints.append(ppo_agent.policy.state_dict())\n",
                "            print(\"Checkpoint saved\")\n",
                "            \n",
                "            # Save model\n",
                "            os.makedirs(\"model_weights_PPO\", exist_ok=True)\n",
                "            ppo_agent.save(f\"model_weights_PPO/{name}_{episode+1}.pth\")\n",
                "    \n",
                "    # Save final model and training statistics\n",
                "    os.makedirs(\"model_weights_random_PPO\", exist_ok=True)\n",
                "    print(f'Saving final model')\n",
                "    torch.save({\n",
                "        'model_state_dict': ppo_agent.policy.state_dict(),\n",
                "        'list_reward_p0': list_reward_p0,\n",
                "        'list_reward_p1': list_reward_p1,\n",
                "        'list_win_count_p0':list_win_count_p0,\n",
                "        'list_win_count_p1':list_win_count_p1, \n",
                "        'list_draw_count': list_draw_count,\n",
                "        'policy_checkpoints': policy_checkpoints,\n",
                "        'num_checkpoints': len(policy_checkpoints)\n",
                "    }, f\"model_weights_random_PPO/{name}_final.pth\")\n",
                "    \n",
                "    env.close()\n",
                "    \n",
                "    return list_reward_p0, list_reward_p1, list_win_count_p0, list_win_count_p1, list_draw_count"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rewards_p0, rewards_p1, win_counts_p0, win_counts_p1, draw_counts = train_ppo_random(\"cathedral_ppo_vs_random\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot results\n",
                "plt.figure(figsize=(15, 10))\n",
                "\n",
                "# Plot rewards for both players\n",
                "plt.subplot(2, 2, 1)\n",
                "plt.plot(rewards_p0, label='Player 0', alpha=0.7)\n",
                "plt.plot(rewards_p1, label='Player 1', alpha=0.7)\n",
                "plt.title(\"Rewards per Episode\")\n",
                "plt.xlabel(\"Episode\")\n",
                "plt.ylabel(\"Total Reward\")\n",
                "plt.legend()\n",
                "\n",
                "# Plot moving average of rewards for better visualization\n",
                "window = 100\n",
                "plt.subplot(2, 2, 2)\n",
                "p0_avg = [np.mean(rewards_p0[max(0, i-window):i+1]) for i in range(len(rewards_p0))]\n",
                "p1_avg = [np.mean(rewards_p1[max(0, i-window):i+1]) for i in range(len(rewards_p1))]\n",
                "plt.plot(p0_avg, label='Player 0 (100-ep avg)', alpha=0.7)\n",
                "plt.plot(p1_avg, label='Player 1 (100-ep avg)', alpha=0.7)\n",
                "plt.title(\"Moving Average Rewards (100 episodes)\")\n",
                "plt.xlabel(\"Episode\")\n",
                "plt.ylabel(\"Average Reward\")\n",
                "plt.legend()\n",
                "\n",
                "# Plot win rates for player 0\n",
                "plt.subplot(2, 2, 3)\n",
                "\n",
                "# Start from episode 1 to avoid division by zero\n",
                "start_idx = 1  # Skip episode 0\n",
                "\n",
                "# Calculate win rates as a function of episodes played\n",
                "total_games = [i + 1 for i in range(len(win_counts_p0))]  # Assuming one entry per game\n",
                "p0_win_rates = [wins / games for wins, games in zip(win_counts_p0, total_games)]\n",
                "p1_win_rates = [wins / games for wins, games in zip(win_counts_p1, total_games)]\n",
                "draw_rates = [draws / games for draws, games in zip(draw_counts, total_games)]\n",
                "\n",
                "# Plot with matching dimensions\n",
                "plt.plot(range(len(p0_win_rates)), p0_win_rates, marker='', label='Player 0 Win Rate', color='blue')\n",
                "plt.plot(range(len(p1_win_rates)), p1_win_rates, marker='', label='Player 1 Win Rate', color='orange')\n",
                "plt.plot(range(len(draw_rates)), draw_rates, marker='', label='Draw Rate', color='green')\n",
                "plt.title(\"Cumulative Win/Draw Rates\")\n",
                "plt.xlabel(\"Episode\")\n",
                "plt.ylabel(\"Rate\")\n",
                "plt.ylim(0, 1)\n",
                "plt.legend()\n",
                "plt.grid(True, linestyle='--', alpha=0.5)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(\"ppo_self_play_training_results.png\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_ppo_against_random(model_path, num_episodes=100, board_size=10, render=False):\n",
                "    \"\"\"\n",
                "    Evaluate a trained PPO agent against a random policy.\n",
                "    \n",
                "    Args:\n",
                "        model_path (str): Path to the saved PPO model weights\n",
                "        num_episodes (int): Number of evaluation episodes\n",
                "        board_size (int): Size of the board\n",
                "        render (bool): Whether to render the game\n",
                "        \n",
                "    Returns:\n",
                "        dict: Evaluation metrics including win rate\n",
                "    \"\"\"\n",
                "    env = cathedral_v0.env(\n",
                "        board_size=board_size, \n",
                "        render_mode=\"ansi\" if render else None, \n",
                "        per_move_rewards=True, \n",
                "        final_reward_score_difference=False\n",
                "    )\n",
                "    env.reset()\n",
                "    \n",
                "    # Get observation shape and action space\n",
                "    player_0 = \"player_0\"  # PPO agent\n",
                "    player_1 = \"player_1\"  # Random agent\n",
                "    n_actions = env.action_space(player_0).n\n",
                "    obs_shape = env.observe(player_0)[\"observation\"].shape\n",
                "    \n",
                "    print(f\"Loading model from {model_path}\")\n",
                "    \n",
                "    # Initialize PPO agent\n",
                "    ppo_agent = PPO(\n",
                "        obs_shape=obs_shape,\n",
                "        action_dim=n_actions,\n",
                "        lr_actor=0.0005, \n",
                "        lr_critic=0.003,\n",
                "        gamma=0.95,\n",
                "        K_epochs=4,\n",
                "        eps_clip=0.2\n",
                "    )\n",
                "    \n",
                "    # Load model weights\n",
                "    checkpoint = torch.load(model_path, weights_only=False)\n",
                "\n",
                "    # Check if this is a dictionary with 'model_state_dict' key (from training function)\n",
                "    if 'model_state_dict' in checkpoint:\n",
                "        ppo_agent.policy.load_state_dict(checkpoint['model_state_dict'])\n",
                "    else:\n",
                "        # Direct state dict (from .save() method)\n",
                "        ppo_agent.policy.load_state_dict(checkpoint)\n",
                "    \n",
                "    # Set to evaluation mode\n",
                "    ppo_agent.policy.eval()\n",
                "    \n",
                "    # Statistics\n",
                "    stats = {\n",
                "        'ppo_wins': 0,\n",
                "        'random_wins': 0,\n",
                "        'draws': 0,\n",
                "        'ppo_rewards': [],\n",
                "        'random_rewards': [],\n",
                "    }\n",
                "    \n",
                "    for episode in range(num_episodes):\n",
                "        env.reset()\n",
                "        done = False\n",
                "        episode_reward_ppo = 0\n",
                "        episode_reward_random = 0\n",
                "        \n",
                "        # Print progress\n",
                "        if episode % 10 == 0:\n",
                "            print(f\"Playing episode {episode}/{num_episodes}\")\n",
                "        \n",
                "        while env.agents:\n",
                "            current_agent = env.agent_selection\n",
                "            \n",
                "            if current_agent == player_1:  # PPO agent's turn\n",
                "                observation = env.observe(current_agent)\n",
                "                state = observation[\"observation\"]\n",
                "                action_mask = observation[\"action_mask\"]\n",
                "                \n",
                "                # Use PPO to select action (deterministic for evaluation)\n",
                "                with torch.no_grad():\n",
                "                    action = ppo_agent.select_action(state, action_mask)\n",
                "                    print(f'PPO action {action}')\n",
                "                \n",
                "            else:  # Random agent's turn\n",
                "                observation = env.observe(current_agent)\n",
                "                action_mask = observation[\"action_mask\"]\n",
                "                \n",
                "                # Random valid action\n",
                "                valid_actions = np.where(action_mask == 1)[0]\n",
                "                action = np.random.choice(valid_actions)\n",
                "                print(f'Random action {action}')\n",
                "            \n",
                "            # Take action\n",
                "            env.step(action)\n",
                "            \n",
                "            # Track rewards\n",
                "            if current_agent == player_1:\n",
                "                episode_reward_ppo += env.rewards[current_agent]\n",
                "            else:\n",
                "                episode_reward_random += env.rewards[current_agent]\n",
                "            \n",
                "        \n",
                "        print(f'Episode {episode} winner {env.winner}')\n",
                "        # Record game outcome\n",
                "        if env.winner == 1:  # PPO agent won\n",
                "            stats['ppo_wins'] += 1\n",
                "        elif env.winner == 0:  # Random agent won\n",
                "            stats['random_wins'] += 1\n",
                "        else:  # Draw\n",
                "            stats['draws'] += 1\n",
                "        \n",
                "        # Record episode rewards\n",
                "        stats['ppo_rewards'].append(episode_reward_ppo)\n",
                "        stats['random_rewards'].append(episode_reward_random)\n",
                "    \n",
                "    # Calculate win rates\n",
                "    stats['ppo_win_rate'] = stats['ppo_wins'] / num_episodes\n",
                "    stats['random_win_rate'] = stats['random_wins'] / num_episodes\n",
                "    stats['draw_rate'] = stats['draws'] / num_episodes\n",
                "    \n",
                "    # Calculate average rewards\n",
                "    stats['avg_ppo_reward'] = sum(stats['ppo_rewards']) / num_episodes\n",
                "    stats['avg_random_reward'] = sum(stats['random_rewards']) / num_episodes\n",
                "    \n",
                "    # Print summary\n",
                "    print(\"\\n===== Evaluation Results =====\")\n",
                "    print(f\"Episodes played: {num_episodes}\")\n",
                "    print(f\"PPO Wins: {stats['ppo_wins']} ({stats['ppo_win_rate']:.2%})\")\n",
                "    print(f\"Random Wins: {stats['random_wins']} ({stats['random_win_rate']:.2%})\")\n",
                "    print(f\"Draws: {stats['draws']} ({stats['draw_rate']:.2%})\")\n",
                "    print(f\"Average PPO Reward: {stats['avg_ppo_reward']:.2f}\")\n",
                "    print(f\"Average Random Reward: {stats['avg_random_reward']:.2f}\")\n",
                "    \n",
                "    env.close()\n",
                "    return stats"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_path = \"model_weights_random_PPO/cathedral_ppo_vs_random_final.pth\"\n",
                "\n",
                "eval_stats = evaluate_ppo_against_random(\n",
                "    model_path=model_path,\n",
                "    num_episodes=100,\n",
                "    board_size=8,\n",
                "    render=False\n",
                ")\n",
                "\n",
                "# Create a bar chart of win rates\n",
                "labels = ['PPO Agent', 'Random Agent', 'Draw']\n",
                "values = [eval_stats['ppo_win_rate'], eval_stats['random_win_rate'], eval_stats['draw_rate']]\n",
                "colors = ['green', 'red', 'gray']\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.bar(labels, values, color=colors)\n",
                "plt.title('Win Rate Comparison')\n",
                "plt.ylabel('Win Rate')\n",
                "plt.ylim(0, 1)\n",
                "\n",
                "for i, v in enumerate(values):\n",
                "    plt.text(i, v + 0.02, f'{v:.2%}', ha='center')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('ppo_vs_random_evaluation.png')\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "cathedral",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
